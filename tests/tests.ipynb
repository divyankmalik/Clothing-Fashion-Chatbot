{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2ea2cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Basic imports successful!\n",
      "✅ NLTK data downloaded!\n",
      "✅ Transformers loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Test imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"✅ Basic imports successful!\")\n",
    "\n",
    "# Cell 2: Install NLTK data (one-time setup)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "print(\"✅ NLTK data downloaded!\")\n",
    "\n",
    "# Cell 3: Test heavy imports\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "print(\"✅ Transformers loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad7747cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-search-results in c:\\users\\divya\\anaconda3\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\divya\\anaconda3\\lib\\site-packages (from google-search-results) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b03d703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'{sys.executable}' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade google-search-results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dce0b1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"title\": \"Converse Chuck Taylor All Star\",\n",
      "    \"price\": \"$65.00\",\n",
      "    \"link\": \"https://www.google.com/shopping/product/269289732613269821?gl=us\",\n",
      "    \"source\": \"Converse\",\n",
      "    \"thumbnail\": \"https://serpapi.com/searches/68b3578ff0114aaae4e8d5da/images/879ed5cb2c0b5d2ad3ed8a3b780303013882091fe7f28e1f1323fe67380f1099.webp\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Converse Chuck Taylor All Star XX-Hi\",\n",
      "    \"price\": \"$100.00\",\n",
      "    \"link\": \"https://www.google.com/shopping/product/11173360778535746770?gl=us\",\n",
      "    \"source\": \"Converse\",\n",
      "    \"thumbnail\": \"https://encrypted-tbn2.gstatic.com/shopping?q=tbn:ANd9GcTYziTslbC6d1b_TpQs1VLF0sUz52j2Iyf8NleEqrmMR1TqFCDgKbH8GZpnn-dVfp9kTuksmxfRk1JkjJJqcPVSwUaZItcxz_XTFvXefYxMhoXMrr-xWwxcdTOS\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Converse Women's Chuck Taylor All Star Lift Platform High\",\n",
      "    \"price\": \"$75.00\",\n",
      "    \"link\": \"https://www.google.com/shopping/product/1255692485203310448?gl=us\",\n",
      "    \"source\": \"Converse\",\n",
      "    \"thumbnail\": \"https://encrypted-tbn1.gstatic.com/shopping?q=tbn:ANd9GcTD5jI7HaYqIlusYdL5VzfUxkl49YQv2Qauo5cfe3OyqMXnc1MYzDlajFkj8OpBFNaW-in0x2Uw_Abi3sAMgrbpDkYlcGw_w-pMz0qsYYCmDJokIzYb_sJUbpo\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Converse Men's Chuck 70\",\n",
      "    \"price\": \"$90.00\",\n",
      "    \"link\": \"https://www.google.com/shopping/product/4267911553313623826?gl=us\",\n",
      "    \"source\": \"Converse\",\n",
      "    \"thumbnail\": \"https://serpapi.com/searches/68b3578ff0114aaae4e8d5da/images/879ed5cb2c0b5d2ad3ed8a3b780303012e08319c8975c42dc99e37a1f8f2a98b.webp\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Converse Men Chuck Taylor All Star\",\n",
      "    \"price\": \"$70.00\",\n",
      "    \"link\": \"https://www.google.com/shopping/product/4737653575385432025?gl=us\",\n",
      "    \"source\": \"Converse\",\n",
      "    \"thumbnail\": \"https://encrypted-tbn1.gstatic.com/shopping?q=tbn:ANd9GcRV6v4ho6ONJjx_ud6_HKa3umUJE8_X-WsBA9ETXlExy9-X37E1OBkzSD85YIiU09vmIyrtYcSBpTqkSSqyXac4u7tIe25-ev1iamdjQlHXMZqQAWJVWn01PAg\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Converse Chuck Taylor All Star CX EXP2\",\n",
      "    \"price\": \"$80.00\",\n",
      "    \"link\": \"https://www.google.com/shopping/product/6489230893609536541?gl=us\",\n",
      "    \"source\": \"Converse\",\n",
      "    \"thumbnail\": \"https://encrypted-tbn1.gstatic.com/shopping?q=tbn:ANd9GcQvLkMKbXsm1AK72Nfl0Y4oa5WpMxU7b094XDMsx3A4A4b97WSXRfvvXch-9nQuA9UtT0la1-M69c8ctjYsmEXZooJrHQKv136EznFnk1MdPPvxJvnOrgiM_0I\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Converse Chuck Taylor All Star Canvas High Top Shoes\",\n",
      "    \"price\": \"$65.00\",\n",
      "    \"link\": \"https://www.google.com/shopping/product/5594245684904411936?gl=us\",\n",
      "    \"source\": \"Converse\",\n",
      "    \"thumbnail\": \"https://encrypted-tbn2.gstatic.com/shopping?q=tbn:ANd9GcRFZvCFBlBFe8pGBsZuZ3F64vcVp6cYnrxbykxPHJeXKG0WalZws-BKrvQ7t1MdbFkx3LGVlhixnV_tDd6cWL7XzYg4-OZFDMszaTa8jS3lqNvAAlFpQC_-b98\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from serpapi.google_search import GoogleSearch\n",
    "import json\n",
    "\n",
    "api_key = \"7d6456df8bfa6b5fa74248f33674305400200d30a378d74111a9457ce2c151eb\"\n",
    "\n",
    "def fetch_products(query, num=10):\n",
    "    params = {\n",
    "        \"engine\": \"google_shopping\",\n",
    "        \"q\": query,\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "        \"api_key\": api_key,\n",
    "        \"num\": num\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "\n",
    "    products = []\n",
    "    for item in results.get(\"shopping_results\", []):\n",
    "        products.append({\n",
    "            \"title\": item.get(\"title\"),\n",
    "            \"price\": item.get(\"price\"),\n",
    "            \"link\": item.get(\"link\") or item.get(\"product_link\"),\n",
    "            \"source\": item.get(\"source\"),\n",
    "            \"thumbnail\": item.get(\"thumbnail\")\n",
    "        })\n",
    "    return products\n",
    "\n",
    "# Example usage\n",
    "products = fetch_products(\"Converse shoes\")\n",
    "print(json.dumps(products[:7], indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "040de61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Product: Converse Chuck Taylor All Star. Price: $65.00. Store: Converse. Link: https://www.google.com/shopping/product/269289732613269821?gl=us', 'Product: Converse Chuck Taylor All Star XX-Hi. Price: $100.00. Store: Converse. Link: https://www.google.com/shopping/product/11173360778535746770?gl=us', \"Product: Converse Women's Chuck Taylor All Star Lift Platform High. Price: $75.00. Store: Converse. Link: https://www.google.com/shopping/product/1255692485203310448?gl=us\"]\n"
     ]
    }
   ],
   "source": [
    "def chunk_products(products, chunk_size=200):\n",
    "    chunks = []\n",
    "    for product in products:\n",
    "        text = f\"Product: {product['title']}. Price: {product['price']}. Store: {product['source']}. Link: {product['link']}\"\n",
    "        # (Simple chunking: if text > chunk_size, split it)\n",
    "        if len(text) > chunk_size:\n",
    "            for i in range(0, len(text), chunk_size):\n",
    "                chunks.append(text[i:i+chunk_size])\n",
    "        else:\n",
    "            chunks.append(text)\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_products(products)\n",
    "print(chunks[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89eff448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: serpapi in c:\\users\\divya\\anaconda3\\lib\\site-packages (0.1.5)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\divya\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\divya\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\divya\\anaconda3\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: pillow in c:\\users\\divya\\anaconda3\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\divya\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\divya\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\divya\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install serpapi tiktoken scikit-learn opencv-python pillow requests numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40dfdcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-search-results in c:\\users\\divya\\anaconda3\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\divya\\anaconda3\\lib\\site-packages (from google-search-results) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58c13c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.5\n"
     ]
    }
   ],
   "source": [
    "import serpapi\n",
    "print(serpapi.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f401ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from serpapi.google_search import GoogleSearch\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "import tiktoken\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "from dataclasses import dataclass, asdict\n",
    "from abc import ABC, abstractmethod\n",
    "import hashlib\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ff8b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"7d6456df8bfa6b5fa74248f33674305400200d30a378d74111a9457ce2c151eb\"\n",
    "\n",
    "def fetch_products(query, num=10):\n",
    "    params = {\n",
    "        \"engine\": \"google_shopping\",\n",
    "        \"q\": query,\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "        \"api_key\": api_key,\n",
    "        \"num\": num\n",
    "    }\n",
    "    \n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    \n",
    "    products = []\n",
    "    for item in results.get(\"shopping_results\", []):\n",
    "        products.append({\n",
    "            \"title\": item.get(\"title\"),\n",
    "            \"price\": item.get(\"price\"),\n",
    "            \"link\": item.get(\"link\") or item.get(\"product_link\"),\n",
    "            \"source\": item.get(\"source\"),\n",
    "            \"thumbnail\": item.get(\"thumbnail\"),\n",
    "            \"rating\": item.get(\"rating\"),\n",
    "            \"reviews\": item.get(\"reviews\"),\n",
    "            \"delivery\": item.get(\"delivery\"),\n",
    "            \"product_id\": item.get(\"product_id\")\n",
    "        })\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b88776fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': \"Nike Men's Dunk Low Retro\", 'price': '$96.97', 'link': 'https://www.google.com/shopping/product/15554707778408471208?gl=us', 'source': 'Nike', 'thumbnail': 'https://serpapi.com/searches/68b35e547dcfbe8cccf78b3a/images/7de47e887a12f0fecca4568b52d9ecd6af2b46e968c3fbfd548c7280b825d689.webp', 'rating': 4.5, 'reviews': 8800, 'delivery': 'Get it by Sep 6 (Free)', 'product_id': '15554707778408471208'}, {'title': \"Nike Men's Dunk Low Retro SE\", 'price': '$110.97', 'link': 'https://www.google.com/shopping/product/2331540138465432860?gl=us', 'source': 'Nike', 'thumbnail': 'https://serpapi.com/searches/68b35e547dcfbe8cccf78b3a/images/7de47e887a12f0fecca4568b52d9ecd61785e96026c5101c8c17781a86b5556c.webp', 'rating': 4.8, 'reviews': 461, 'delivery': 'Get it by Sep 6 (Free)', 'product_id': '2331540138465432860'}, {'title': \"Men's Nike Dunk High Retro\", 'price': '$131.97', 'link': 'https://www.google.com/shopping/product/9754079018287107983?gl=us', 'source': 'Nike', 'thumbnail': 'https://serpapi.com/searches/68b35e547dcfbe8cccf78b3a/images/7de47e887a12f0fecca4568b52d9ecd64a719a2be40f4ef031bfc5a2c0175584.webp', 'rating': 4.6, 'reviews': 1200, 'delivery': 'Get it by Sep 6 (Free)', 'product_id': '9754079018287107983'}, {'title': \"Nike Women's Dunk High\", 'price': '$94.50', 'link': 'https://www.google.com/shopping/product/12164606461305287017?gl=us', 'source': 'Nordstrom', 'thumbnail': 'https://serpapi.com/searches/68b35e547dcfbe8cccf78b3a/images/7de47e887a12f0fecca4568b52d9ecd639e526401e5af49ea88c9c6bb4a27df3.webp', 'rating': 4.5, 'reviews': 966, 'delivery': 'Get it by Sep 9 (Free)', 'product_id': '12164606461305287017'}, {'title': \"Nike Women's Dunk Low Twist Sneakers\", 'price': '$76.00', 'link': 'https://www.google.com/shopping/product/10778117412342486137?gl=us', 'source': 'Lyst', 'thumbnail': 'https://serpapi.com/searches/68b35e547dcfbe8cccf78b3a/images/7de47e887a12f0fecca4568b52d9ecd6ba29ede3da3e997f1e3d9256e1ddfa43.webp', 'rating': 4.2, 'reviews': 268, 'delivery': None, 'product_id': '10778117412342486137'}, {'title': \"Nike Women's Dunk Low\", 'price': '$79.00', 'link': 'https://www.google.com/shopping/product/8566382533949587599?gl=us', 'source': 'GOAT', 'thumbnail': 'https://serpapi.com/searches/68b35e547dcfbe8cccf78b3a/images/7de47e887a12f0fecca4568b52d9ecd671b0b22363f21bf95840be730eb82cb1.webp', 'rating': 4.5, 'reviews': 6800, 'delivery': None, 'product_id': '8566382533949587599'}, {'title': \"Women's Nike Dunk Low\", 'price': '$149.00', 'link': 'https://www.google.com/shopping/product/8766255114553396384?gl=us', 'source': 'Flight Club', 'thumbnail': 'https://encrypted-tbn1.gstatic.com/shopping?q=tbn:ANd9GcSi2H4nTTfI9ReQPqwUgKlhLPoBLTHKqYSaOK1aDKWvK5KUo5N-mzgFhNrPuqFab9CHqMylg9iVzCAIX0aS37ZLOva4KCha_Go-rGsM2T3lBI1A2vfPxnxXRw', 'rating': 4.5, 'reviews': 8800, 'delivery': None, 'product_id': '8766255114553396384'}, {'title': \"Nike Men's Dunk Low Retro SE Basketball Sneakers\", 'price': '$130.00', 'link': 'https://www.google.com/shopping/product/11746003715507511813?gl=us', 'source': 'cncpts.com', 'thumbnail': 'https://encrypted-tbn1.gstatic.com/shopping?q=tbn:ANd9GcQTP8OcB_vrh9866BzyEp2b4NGwDkboPlk4f_Ltk2AwAigFTwE3YlLrHoU9Jr2_2AXDQa6axAxIITNqDhelgUV3ci6Kvwvtt1gY2gc1Gp4a34b9RvzOC6W-TkQ', 'rating': 4.0, 'reviews': 4, 'delivery': None, 'product_id': '11746003715507511813'}, {'title': 'Nike Dunk Low Retro', 'price': '$94.00', 'link': 'https://www.google.com/shopping/product/6016245521608251857?gl=us', 'source': 'Lyst', 'thumbnail': 'https://encrypted-tbn3.gstatic.com/shopping?q=tbn:ANd9GcRId7-d-nCKKHsVBXkvdqxSop2S0ZCx1IUQDfz0U_AcpeBhDsnA55xIMQJdIDqkTa98k7QWK86zfGy4aqXvyEtbML4AJFomR_tawgSt5TtfQuxQhaFsqlDdFg', 'rating': 4.7, 'reviews': 597, 'delivery': None, 'product_id': '6016245521608251857'}, {'title': 'Nike Dunk Low Panda (2021)', 'price': '$130.00', 'link': 'https://www.google.com/shopping/product/17533191351008967883?gl=us', 'source': 'Common Hype', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/shopping?q=tbn:ANd9GcTUYJ6QWLdufwfMc7i3qXyi8ev6A9cW2_o3pKcncE1nCxnrHRdeWkLMoGHIGUzptwvZDC2eR1POey0LThFxlv9ScgGfAsNsml4F0k-brM7pcC5B8BwqR3AZvA', 'rating': None, 'reviews': None, 'delivery': None, 'product_id': '17533191351008967883'}, {'title': \"Nike Men's Dunk Low Retro '80s Color Block Panda Leather Shoes\", 'price': '$69.99', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:15653139493800915155', 'source': 'eBay - the.deal.ninja', 'thumbnail': 'https://encrypted-tbn3.gstatic.com/shopping?q=tbn:ANd9GcTOzeU_ZL4u0DGL8jcC3V-gPvyw8_mgfGSDfBOXZAHjVbbupPiU7VuL6qqIA3_tBR9P-F7E7Rb3wo2MLqfzJiLl1TOMQA2lVClT6kQMG2EW6QQcXM8XXaKnPA', 'rating': 4.6, 'reviews': 1900, 'delivery': 'Free delivery', 'product_id': '15653139493800915155'}, {'title': \"Nike Men's Dunk Low Reverse Panda\", 'price': '$308.00', 'link': 'https://www.google.com/shopping/product/6069410001458166056?gl=us', 'source': 'Flight Club', 'thumbnail': 'https://serpapi.com/searches/68b35e547dcfbe8cccf78b3a/images/7de47e887a12f0fecca4568b52d9ecd684e7f0cf4d7f14dc4edcf0560ad89e1c.webp', 'rating': 4.6, 'reviews': 195, 'delivery': None, 'product_id': '6069410001458166056'}, {'title': \"Nike Men's Dunk Low\", 'price': '$1,611.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:12777498940010684347', 'source': 'GOAT', 'thumbnail': 'https://encrypted-tbn1.gstatic.com/shopping?q=tbn:ANd9GcSgtp6TiUYangS2yW-n2xTIQWW_BgeG8V-sbs4b3NT8gghK7IdtLOID0oy1pCkWvmTi9wqrvoVN1ytINTeD-LY_JvWuHFLR8vtM1atoyZJ7BFSuAf1_tHXA', 'rating': 4.5, 'reviews': 8800, 'delivery': None, 'product_id': '12777498940010684347'}, {'title': \"Nike Men's Dunk High\", 'price': '$116.00', 'link': 'https://www.google.com/shopping/product/3862728506500217219?gl=us', 'source': 'Lyst', 'thumbnail': 'https://encrypted-tbn3.gstatic.com/shopping?q=tbn:ANd9GcTWgWEAEcgTVMpZFR-pxw6cwyQMM8QJdON1i3jsAN3CNnoBTp8trxRcUHj2kAc2mjhu6901Z8Yaq8dERrRkm_sD_cVb_WvoEfHX9e19QO-rL30OhroE5qav', 'rating': 4.5, 'reviews': 871, 'delivery': None, 'product_id': '3862728506500217219'}, {'title': \"Nike Women's Dunk Low Retro Lifestyle Shoes\", 'price': '$120.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:9498698370357357995', 'source': 'Shoe Palace', 'thumbnail': 'https://encrypted-tbn3.gstatic.com/shopping?q=tbn:ANd9GcSrzym_G1xIIsRjzs3RT1hfnIIrql3BNkca5KFxHyum9eP33HZF0pyURB8vK2tWn4NB1CoIvJDeUAinwQrMsujkhiJFApTV5Xc_uBaORFfkQrU4d8qZJM8grg', 'rating': 4.5, 'reviews': 6800, 'delivery': None, 'product_id': '9498698370357357995'}, {'title': \"Nike Women's Dunk Low Leather Sneakers\", 'price': '$83.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:14872967861491880474', 'source': 'farfetch.com', 'thumbnail': 'https://serpapi.com/searches/68b35e547dcfbe8cccf78b3a/images/7de47e887a12f0fecca4568b52d9ecd6968f86b67e811df5476e585f61056f61.webp', 'rating': 4.5, 'reviews': 8800, 'delivery': None, 'product_id': '14872967861491880474'}, {'title': 'Nike Dunk Low Retro Sneakers', 'price': '$96.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:14813069101912186988', 'source': 'ASOS', 'thumbnail': 'https://encrypted-tbn1.gstatic.com/shopping?q=tbn:ANd9GcQd_KjOiHo4DnE5z-j4FPB-W1g20w49QStSdwoxzdsyo1mazu3d_S_azLOs7BhaWYbO-oq49WflXZInHLHShQhDe6CkSzqH3bX9xyC8MJLdRoQksGi1az2vsw', 'rating': 4.7, 'reviews': 597, 'delivery': 'Get it by Sep 11 (Free)', 'product_id': '14813069101912186988'}, {'title': 'Nike Dunk Low Retro Sneakers', 'price': '$130.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:576655623897196006', 'source': 'ssense.com', 'thumbnail': 'https://encrypted-tbn2.gstatic.com/shopping?q=tbn:ANd9GcRGTniX2jgHNrOBaokcQYKtci6fEwo3vmB9ixeBIP1pm-N0WnzgxbBhF9Agf6rpRxJeleu1NgSKYam_voNXEDH2qGXhmT8pcg6b2VyWFi80FI2-2quP3Tr3', 'rating': 4.5, 'reviews': 8800, 'delivery': 'Get it by Thu (Free)', 'product_id': '576655623897196006'}, {'title': 'Dunk Low Retro Sneakers', 'price': '$154.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:6054132918998457707', 'source': 'Slam Jam', 'thumbnail': 'https://encrypted-tbn3.gstatic.com/shopping?q=tbn:ANd9GcS8JqNMDW6oFchDe1w1td2XUKy_-qvrD1ZfyKRWaWjJyRgcRO7MPcqvXwWArEePSh43zvXbyI9CuZ2NCHVjJ2ZGqr0N_JA6EdiRJVfd2cJ9LRg9kY2JW9Q2IQ', 'rating': 4.5, 'reviews': 8800, 'delivery': 'Free delivery on $350+', 'product_id': '6054132918998457707'}, {'title': \"Men's Nike Dunk Mid Panda\", 'price': '$71.00', 'link': 'https://www.google.com/shopping/product/17376923186175212858?gl=us', 'source': 'Flight Club', 'thumbnail': 'https://serpapi.com/searches/68b35e547dcfbe8cccf78b3a/images/7de47e887a12f0fecca4568b52d9ecd610f22d39897bc12a82b94df3e9636796.webp', 'rating': 4.4, 'reviews': 19, 'delivery': None, 'product_id': '17376923186175212858'}, {'title': \"Nike Women's Dunk Low Black/ White Panda Fashion Shoes\", 'price': '$71.99', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:13809226320795002727', 'source': 'eBay - poofyo101jumpmansneakers', 'thumbnail': 'https://encrypted-tbn2.gstatic.com/shopping?q=tbn:ANd9GcRdLDDn-I53TX6Ooafwe0dv2o5Ew38kwLzt7cBDXuPcoqEEC3RhiZOksGQgX3YlRCs9jZILNg9Ibx0P2L5s8Xw9Z8FWFiFX8b8JfYtV2X9JKsEvCx1Z20FSLNBF', 'rating': 5.0, 'reviews': 6, 'delivery': 'Get it by Sep 12 (Free)', 'product_id': '13809226320795002727'}, {'title': \"Nike Women's Dunk Low\", 'price': '$1,372.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:8269232218516837697', 'source': 'GOAT', 'thumbnail': 'https://encrypted-tbn3.gstatic.com/shopping?q=tbn:ANd9GcQCX8RbUjLjhDKLoQipkQyKsuZ8vsMmD8DKpwaJKk-ZX9G1PqqmI1mK8oa-8qtL6yziyaI6JOd31UCEGM6voiEtbZVr3R7eldjHoKpTQTF7aTGKL3tIZYE2', 'rating': 4.5, 'reviews': 6800, 'delivery': None, 'product_id': '8269232218516837697'}, {'title': 'Nike Dunk Low Retro Panda Sneakers', 'price': '$77.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:3901716450562615231', 'source': 'farfetch.com', 'thumbnail': 'https://encrypted-tbn3.gstatic.com/shopping?q=tbn:ANd9GcQNvJRHPpXskahb4VHuF8fOvezQbrU3HcaO3S-O19JShZTv4-Ji2I1efbqw4DNrhTie7FWopW_CPIJlhHnLHenAaUssqKtSy1lQN8wdF8lJ1F4q3wbhul3y6Q', 'rating': 4.7, 'reviews': 64, 'delivery': None, 'product_id': '3901716450562615231'}, {'title': 'Nike Dunk Low Retro Panda', 'price': '$110.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:12343208131007068713', 'source': 'ShopSimon', 'thumbnail': 'https://encrypted-tbn2.gstatic.com/shopping?q=tbn:ANd9GcSJTDpgzTOmy-1EtLaf1X4fdED1xch9ZWwH-Y_EV0W_x5xy_x3GYxB8h8kE6ETzbfypLg3gbyfZlvvX4_S0DV2tM2FQ-4Gl_ezGnOXN2iZiK-aIieJBXCx3', 'rating': 4.6, 'reviews': 159, 'delivery': None, 'product_id': '12343208131007068713'}, {'title': 'Nike Dunk Low Sneakers', 'price': '$115.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:3985833239440229939', 'source': 'ssense.com', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/shopping?q=tbn:ANd9GcQ4Yn9Ijw2Fers89XfQ7KElFICwRCLm0ySP7kQZ6O5JPQE7uHRmFEgHshTWi9MLoaLSvWqCF0tAFx9QCy00f0KCmuUrN6igF-JVjAEOKatlvGO8Z0N8UEx1', 'rating': 4.9, 'reviews': 188, 'delivery': 'Get it by Thu (Free)', 'product_id': '3985833239440229939'}, {'title': \"Nike Women's Dunk Low\", 'price': '$62.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:15425393922549567424', 'source': 'Flight Club', 'thumbnail': 'https://serpapi.com/searches/68b35e547dcfbe8cccf78b3a/images/7de47e887a12f0fecca4568b52d9ecd664e9969c1a830bd78368c8e52969ccf0.webp', 'rating': 4.5, 'reviews': 6800, 'delivery': None, 'product_id': '15425393922549567424'}, {'title': 'Nike Dunk Low Panda Sneakers', 'price': '$92.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:17610377959604759595', 'source': 'ASOS', 'thumbnail': 'https://encrypted-tbn3.gstatic.com/shopping?q=tbn:ANd9GcRTFz7_Syd4Btor9U58P8X4cLiYROkDYrDISoXeXvIM90yrrw1FbOMyf20sVo9apWKlJqL1CHcaCsY8h2PFKJaqn7HTBZouAfM89gESaueE110_rhce-Xmstg', 'rating': None, 'reviews': None, 'delivery': 'Get it by Sep 11 (Free)', 'product_id': '17610377959604759595'}, {'title': \"Nike Women's Dunk Low Retro '80s Color Block Panda Leather Shoes\", 'price': '$64.99', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:17975058047042872186', 'source': 'eBay - the.deal.ninja', 'thumbnail': 'https://encrypted-tbn3.gstatic.com/shopping?q=tbn:ANd9GcTFANafC7txK259ndqUI-Br9KyJH-BjCx7HsOaYuejYiX_in5xfMmk--8rA420NWk6X--_hw33uSCRSVmm6iI3O_QCKkyUEvb4bHmrZcQGMZTSINBn2Lm2o', 'rating': 5.0, 'reviews': 6, 'delivery': 'Free delivery', 'product_id': '17975058047042872186'}, {'title': 'Nike Dunk Low Retro SE Sneakers', 'price': '$115.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:1297082791586086866', 'source': 'Sneak in Peace', 'thumbnail': 'https://encrypted-tbn3.gstatic.com/shopping?q=tbn:ANd9GcTUzO4wX15cG-VdTCvjBj_drrGeWB6JuRzuH8nC9C-j-yHJAy90Q4Q81CO-VK4n2NS7MEaTHH886-WkUivFzwzqVbx9etVLP5QwtDqkaEDL9InZNDHdOKWjS8L-', 'rating': None, 'reviews': None, 'delivery': 'Get it by Wed (Free)', 'product_id': '1297082791586086866'}, {'title': 'Nike Dunk LOW EU 37', 'price': '$218.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:3740581147079255227', 'source': 'Vestiaire Collective', 'thumbnail': 'https://encrypted-tbn1.gstatic.com/shopping?q=tbn:ANd9GcQlTjZyluQsi6dXUSATLbeeCDzUBzpxSjX2e7KvG53OSDo5NFeAGn7ptk7IbIickPYgyh5s3wASC_kZx87JJwo3dia4RArT33jlmyi0bhWHOBOOfKNYLU35', 'rating': 4.6, 'reviews': 141, 'delivery': None, 'product_id': '3740581147079255227'}, {'title': 'Nike Dunk Low \"Black/White', 'price': '$159.98', 'link': 'https://www.google.com/shopping/product/14329535674516984651?gl=us', 'source': 'Nine6ixe', 'thumbnail': 'https://encrypted-tbn2.gstatic.com/shopping?q=tbn:ANd9GcQJaCMdrLb5z4JyHYaKROiO20oiJ62IioEjKV7t6Lb1AgVuCxWBh__L0TerLno1tdKBTXxwhE7nhYo6mF50yRu3w8F_ELb-wnrgx8np89xImZnn9UpmKpOOfHIw', 'rating': 4.9, 'reviews': 188, 'delivery': 'Free delivery', 'product_id': '14329535674516984651'}, {'title': \"Nike Women's Dunk Low Next Nature Sockliner\", 'price': '$124.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:16511457762987778172', 'source': 'GOAT', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/shopping?q=tbn:ANd9GcSCSJzPD47RzQHt_bKdhJ2AP5rofKSBvhgJp2WdRErT_RY59iQ3zSIsqmQWZFBUXHR6bvc4HwUjQg6jN4-xnPXYgj3b7NK6KUpinfDkid20XaZCBaByd0WK', 'rating': 4.5, 'reviews': 1600, 'delivery': None, 'product_id': '16511457762987778172'}, {'title': \"Nike Men's Dunk Low Retro Panda\", 'price': '$69.99', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:11010005856434533537', 'source': 'eBay - tmsales2014', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/shopping?q=tbn:ANd9GcSdSu7mk82kiB-FoysKPpcJexf8WwIBPKTKgdWp-AA2huuVXxl60WMAXbawLFweJV2TRzstGkr8wv0u06GDvSEMXxFCkiia8d-WNdbDw-Okz4R_yLRtnV4QtXo', 'rating': 4.7, 'reviews': 597, 'delivery': 'Free delivery', 'product_id': '11010005856434533537'}, {'title': \"Men's Dunk Low Panda Retro\", 'price': '$100.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:1415763837235843743', 'source': 'Blue&Cream', 'thumbnail': 'https://encrypted-tbn3.gstatic.com/shopping?q=tbn:ANd9GcTWJG4ctcmXYWojRG8BtZ_M35hMuHDsDQYuK0vsA90UYngKBU6sysAp40XlmJSZIyoW8GUKUhs0RVjKnG1Hd5_jZAnvkzi5im6eFn6qLP8V', 'rating': 4.5, 'reviews': 8800, 'delivery': 'Free delivery', 'product_id': '1415763837235843743'}, {'title': \"Nike Men's Dunk Low SE Panda-Monium Pack\", 'price': '$149.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:17045217114641350320', 'source': 'Flight Club', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/shopping?q=tbn:ANd9GcSNHdGSf0T7XktYLxrRt0k2zDcsUeuM8f74TKF5WazSfDh0mghu1CsxEzs9V8eMjaZM4nJ3VBLUd8artKpzW1hRr74TykfgVP6pG7sjUWG5n4K8ZBhReB69Tw', 'rating': 4.0, 'reviews': 4, 'delivery': None, 'product_id': '17045217114641350320'}, {'title': \"Nike Men's Panda Dunks Shoes\", 'price': '$25.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:6852572479605797383', 'source': 'Poshmark', 'thumbnail': 'https://encrypted-tbn2.gstatic.com/shopping?q=tbn:ANd9GcS4umqzVfL00aLE5RDqeRvLkitvDqe2gyzUx6SLcRip9nfytjapEnWTlmXUQOifLFCM8h322mJDKahwLnLbcKaowklIgeV5eg', 'rating': 4.5, 'reviews': 95, 'delivery': None, 'product_id': '6852572479605797383'}, {'title': 'Nike Dunk Low Retro Panda Black', 'price': '$110.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:1366412162479356850', 'source': 'Stylight Inc.', 'thumbnail': 'https://encrypted-tbn3.gstatic.com/shopping?q=tbn:ANd9GcTqAWVFo9WvzA9VFvGD5MNfJtjM2vabutvPikPJMO1V1SOY8XE0ukZ3IZIu60DzFy7I1zBOO_rrWe9YJn693nosQoXO5f8g', 'rating': 4.6, 'reviews': 159, 'delivery': 'Free delivery', 'product_id': '1366412162479356850'}, {'title': \"Nike Women's Dunk Low Panda\", 'price': '$165.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:3822795715829517622', 'source': 'eBay - badsneakersjp', 'thumbnail': 'https://encrypted-tbn2.gstatic.com/shopping?q=tbn:ANd9GcQJ0W950l_jn3Z-W-33--k1gn8jWWE0LKu00P7jzP3wDHYAUxqS14Eo7foyxJZSHJiYzvX47kzYXRqcN3kDpw0zRYPBGk3fJ5Adsub7OpIOCgUrTZMQ3-S8ng', 'rating': None, 'reviews': None, 'delivery': 'Free delivery', 'product_id': '3822795715829517622'}, {'title': 'Nike Dunk LOW UK 5', 'price': '$59.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:13524656274379039943', 'source': 'Vestiaire Collective', 'thumbnail': 'https://encrypted-tbn1.gstatic.com/shopping?q=tbn:ANd9GcRn20TwAZLjZx7QKylm4yJ1t1PItu9bwuDRLZvUzkVCGl53Wp1hV60J63YbVZ81or7tqfDa56vgftiX5OFiBRkJluf6oEQa5zxTTlsKaLK2dSYqYL2iUSDn', 'rating': None, 'reviews': None, 'delivery': None, 'product_id': '13524656274379039943'}, {'title': \"Nike Women's Dunk Low Next Nature Sockliner\", 'price': '$132.00', 'link': 'https://www.google.com/shopping/product/1?gl=us&prds=pid:14932365870123076309', 'source': 'GOAT', 'thumbnail': 'https://encrypted-tbn1.gstatic.com/shopping?q=tbn:ANd9GcTwgETak9gm61oZ5Hh-xe-QZYVAYKuMxlxA-LukAOVNa5wIiFPCyxPZbhi11m0bvpdDjiyJ5HlVBGkX_a5w64Lo3wFx6NREOjpa-260MTcqQlIP7o-A-dO7', 'rating': None, 'reviews': None, 'delivery': None, 'product_id': '14932365870123076309'}]\n"
     ]
    }
   ],
   "source": [
    "## testing the function fetch_products\n",
    "print(fetch_products(\"nike shoes dunks with a black and white color combination\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85e22191",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProductChunk:\n",
    "    \"\"\"Product-specific chunk\"\"\"\n",
    "    content: str\n",
    "    chunk_id: str\n",
    "    product_id: str\n",
    "    chunk_type: str  # 'title', 'description', 'specs', 'combined'\n",
    "    metadata: Dict\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "    image_embedding: Optional[np.ndarray] = None\n",
    "\n",
    "@dataclass\n",
    "class ProductImageChunk:\n",
    "    \"\"\"Product image chunk\"\"\"\n",
    "    content: np.ndarray\n",
    "    chunk_id: str\n",
    "    product_id: str\n",
    "    image_url: str\n",
    "    metadata: Dict\n",
    "    embedding: Optional[np.ndarray] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "905f8246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductChunkingStrategy:\n",
    "    \"\"\"Specialized chunking for product data\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\", chunk_size: int = 256):\n",
    "        self.encoder = tiktoken.encoding_for_model(model)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.text_vectorizer = TfidfVectorizer(max_features=384, stop_words='english')\n",
    "        self._vectorizer_fitted = False\n",
    "    \n",
    "    def chunk_product(self, product: Dict) -> List[ProductChunk]:\n",
    "        \"\"\"Chunk a single product into multiple text chunks\"\"\"\n",
    "        chunks = []\n",
    "        product_id = product.get('product_id', '') or hashlib.md5(str(product).encode()).hexdigest()[:8]\n",
    "        \n",
    "        # Title chunk\n",
    "        title = product.get('title', '')\n",
    "        if title:\n",
    "            chunks.append(self._create_chunk(\n",
    "                content=title,\n",
    "                product_id=product_id,\n",
    "                chunk_type='title',\n",
    "                metadata={'price': product.get('price'), 'source': product.get('source')}\n",
    "            ))\n",
    "        \n",
    "        # Description/combined chunk (if we had description, we'd chunk it separately)\n",
    "        # For now, combine available text fields\n",
    "        combined_text = []\n",
    "        if title:\n",
    "            combined_text.append(f\"Title: {title}\")\n",
    "        if product.get('price'):\n",
    "            combined_text.append(f\"Price: {product.get('price')}\")\n",
    "        if product.get('source'):\n",
    "            combined_text.append(f\"Available at: {product.get('source')}\")\n",
    "        if product.get('delivery'):\n",
    "            combined_text.append(f\"Delivery: {product.get('delivery')}\")\n",
    "        if product.get('rating'):\n",
    "            combined_text.append(f\"Rating: {product.get('rating')}\")\n",
    "        if product.get('reviews'):\n",
    "            combined_text.append(f\"Reviews: {product.get('reviews')}\")\n",
    "        \n",
    "        combined_content = \" | \".join(combined_text)\n",
    "        \n",
    "        if combined_content and combined_content != f\"Title: {title}\":\n",
    "            chunks.append(self._create_chunk(\n",
    "                content=combined_content,\n",
    "                product_id=product_id,\n",
    "                chunk_type='combined',\n",
    "                metadata={\n",
    "                    'price': product.get('price'),\n",
    "                    'source': product.get('source'),\n",
    "                    'link': product.get('link'),\n",
    "                    'thumbnail': product.get('thumbnail')\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_chunk(self, content: str, product_id: str, chunk_type: str, metadata: Dict) -> ProductChunk:\n",
    "        \"\"\"Create a product chunk with metadata\"\"\"\n",
    "        chunk_id = hashlib.md5(f\"{product_id}_{chunk_type}_{content}\".encode()).hexdigest()\n",
    "        token_count = len(self.encoder.encode(content))\n",
    "        \n",
    "        metadata.update({\n",
    "            'token_count': token_count,\n",
    "            'char_count': len(content),\n",
    "            'chunk_type': chunk_type\n",
    "        })\n",
    "        \n",
    "        return ProductChunk(\n",
    "            content=content,\n",
    "            chunk_id=chunk_id,\n",
    "            product_id=product_id,\n",
    "            chunk_type=chunk_type,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    \n",
    "    def generate_text_embeddings(self, chunks: List[ProductChunk]) -> List[ProductChunk]:\n",
    "        \"\"\"Generate embeddings for text chunks\"\"\"\n",
    "        if not chunks:\n",
    "            return chunks\n",
    "        \n",
    "        # Fit vectorizer on all chunk content\n",
    "        all_content = [chunk.content for chunk in chunks]\n",
    "        self.text_vectorizer.fit(all_content)\n",
    "        self._vectorizer_fitted = True\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.text_vectorizer.transform(all_content).toarray()\n",
    "        \n",
    "        for chunk, embedding in zip(chunks, embeddings):\n",
    "            chunk.embedding = embedding\n",
    "        \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6ea8232",
   "metadata": {},
   "outputs": [],
   "source": [
    "##testing product chunking strategy(the class above)\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class ProductChunk:\n",
    "    content: str\n",
    "    chunk_id: str\n",
    "    product_id: str\n",
    "    chunk_type: str\n",
    "    metadata: Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0faecb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- CHUNK -----\n",
      "Content: Nike Air Max Shoes\n",
      "Chunk ID: 83aeea942ea8f806f01c315bb5de745d\n",
      "Product ID: 123abc\n",
      "Type: title\n",
      "Metadata: {'price': '$120', 'source': 'Nike', 'token_count': 4, 'char_count': 18, 'chunk_type': 'title'}\n",
      "\n",
      "----- CHUNK -----\n",
      "Content: Title: Nike Air Max Shoes | Price: $120 | Available at: Nike | Delivery: Free shipping | Rating: 4.7 | Reviews: 215\n",
      "Chunk ID: 449ddf450610f01ed8f170263dfff853\n",
      "Product ID: 123abc\n",
      "Type: combined\n",
      "Metadata: {'price': '$120', 'source': 'Nike', 'link': 'https://nike.com/airmax', 'thumbnail': 'https://nike.com/airmax.jpg', 'token_count': 33, 'char_count': 115, 'chunk_type': 'combined'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "import tiktoken\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define ProductChunk (since you didn’t show it)\n",
    "@dataclass\n",
    "class ProductChunk:\n",
    "    content: str\n",
    "    chunk_id: str\n",
    "    product_id: str\n",
    "    chunk_type: str\n",
    "    metadata: Dict\n",
    "\n",
    "# Your ProductChunkingStrategy class here (copy/paste your code)\n",
    "\n",
    "# -------- Test the class --------\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample product dict (simulate API response)\n",
    "    sample_product = {\n",
    "        \"title\": \"Nike Air Max Shoes\",\n",
    "        \"price\": \"$120\",\n",
    "        \"link\": \"https://nike.com/airmax\",\n",
    "        \"source\": \"Nike\",\n",
    "        \"thumbnail\": \"https://nike.com/airmax.jpg\",\n",
    "        \"rating\": 4.7,\n",
    "        \"reviews\": 215,\n",
    "        \"delivery\": \"Free shipping\",\n",
    "        \"product_id\": \"123abc\"\n",
    "    }\n",
    "\n",
    "    # Initialize chunking strategy\n",
    "    chunker = ProductChunkingStrategy(model=\"gpt-3.5-turbo\", chunk_size=256)\n",
    "\n",
    "    # Get chunks\n",
    "    chunks = chunker.chunk_product(sample_product)\n",
    "\n",
    "    # Print results\n",
    "    for c in chunks:\n",
    "        print(\"----- CHUNK -----\")\n",
    "        print(\"Content:\", c.content)\n",
    "        print(\"Chunk ID:\", c.chunk_id)\n",
    "        print(\"Product ID:\", c.product_id)\n",
    "        print(\"Type:\", c.chunk_type)\n",
    "        print(\"Metadata:\", c.metadata)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51ef85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4034fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductImageProcessor:\n",
    "    \"\"\"Process product images\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "    \n",
    "    def download_image(self, url: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"Download and convert image to numpy array\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            image = Image.open(io.BytesIO(response.content))\n",
    "            image = image.convert('RGB')\n",
    "            image_array = np.array(image)\n",
    "            \n",
    "            return image_array\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download image from {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_product_images(self, products: List[Dict]) -> List[ProductImageChunk]:\n",
    "        \"\"\"Process images from products\"\"\"\n",
    "        image_chunks = []\n",
    "        \n",
    "        for product in products:\n",
    "            thumbnail_url = product.get('thumbnail')\n",
    "            if not thumbnail_url:\n",
    "                continue\n",
    "            \n",
    "            image_array = self.download_image(thumbnail_url)\n",
    "            if image_array is None:\n",
    "                continue\n",
    "            \n",
    "            product_id = product.get('product_id', '') or hashlib.md5(str(product).encode()).hexdigest()[:8]\n",
    "            chunk_id = hashlib.md5(f\"{product_id}_{thumbnail_url}\".encode()).hexdigest()\n",
    "            \n",
    "            # Generate simple image embedding (color histogram)\n",
    "            embedding = self._generate_image_embedding(image_array)\n",
    "            \n",
    "            image_chunk = ProductImageChunk(\n",
    "                content=image_array,\n",
    "                chunk_id=chunk_id,\n",
    "                product_id=product_id,\n",
    "                image_url=thumbnail_url,\n",
    "                metadata={\n",
    "                    'image_shape': image_array.shape,\n",
    "                    'product_title': product.get('title'),\n",
    "                    'product_price': product.get('price')\n",
    "                },\n",
    "                embedding=embedding\n",
    "            )\n",
    "            \n",
    "            image_chunks.append(image_chunk)\n",
    "        \n",
    "        return image_chunks\n",
    "    \n",
    "    def _generate_image_embedding(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Generate embedding for image using color histogram\"\"\"\n",
    "        # Convert to BGR for OpenCV\n",
    "        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Calculate color histograms\n",
    "        hist_b = cv2.calcHist([image_bgr], [0], None, [32], [0, 256]).flatten()\n",
    "        hist_g = cv2.calcHist([image_bgr], [1], None, [32], [0, 256]).flatten()\n",
    "        hist_r = cv2.calcHist([image_bgr], [2], None, [32], [0, 256]).flatten()\n",
    "        \n",
    "        # Combine histograms\n",
    "        embedding = np.concatenate([hist_b, hist_g, hist_r])\n",
    "        \n",
    "        # Normalize\n",
    "        embedding = embedding / (np.linalg.norm(embedding) + 1e-8)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "edddfc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing product image processor function part 1\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class ProductImageChunk:\n",
    "    content: np.ndarray     # the raw image array\n",
    "    chunk_id: str\n",
    "    product_id: str\n",
    "    image_url: str\n",
    "    metadata: Dict\n",
    "    embedding: np.ndarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6506dc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Image Chunk 1 ===\n",
      "Product ID: museum-nike-2\n",
      "Chunk ID: 2ba2ae1000ac851e85141ef9ac15e3fa\n",
      "Image URL: https://upload.wikimedia.org/wikipedia/commons/6/6a/Nike_shoes_2.jpg\n",
      "Metadata: {'image_shape': (2448, 3264, 3), 'product_title': 'Nike shoes (museum photo)', 'product_price': '$110'}\n",
      "Embedding shape: (96,)\n",
      "First 10 embedding values: [0.07369487 0.10861412 0.1089415  0.10688066 0.09883226 0.12858525\n",
      " 0.16849989 0.1805296  0.1763189  0.1819615 ]\n"
     ]
    }
   ],
   "source": [
    "## testing product image processor function part 2\n",
    "import hashlib\n",
    "import io\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Your ProductImageProcessor class goes here\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample product list with a real image URL\n",
    "    sample_products = [{\n",
    "    \"title\": \"Nike shoes (museum photo)\",\n",
    "    \"price\": \"$110\",\n",
    "    \"thumbnail\": \"https://upload.wikimedia.org/wikipedia/commons/6/6a/Nike_shoes_2.jpg\",\n",
    "    \"product_id\": \"museum-nike-2\"\n",
    "    }]\n",
    "\n",
    "\n",
    "    # Initialize processor\n",
    "    processor = ProductImageProcessor()\n",
    "\n",
    "    # Process product images\n",
    "    image_chunks = processor.process_product_images(sample_products)\n",
    "\n",
    "    # Print results\n",
    "    for idx, chunk in enumerate(image_chunks, 1):\n",
    "        print(f\"\\n=== Image Chunk {idx} ===\")\n",
    "        print(\"Product ID:\", chunk.product_id)\n",
    "        print(\"Chunk ID:\", chunk.chunk_id)\n",
    "        print(\"Image URL:\", chunk.image_url)\n",
    "        print(\"Metadata:\", chunk.metadata)\n",
    "        print(\"Embedding shape:\", chunk.embedding.shape)\n",
    "        print(\"First 10 embedding values:\", chunk.embedding[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4923c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductEmbeddingStore:\n",
    "    \"\"\"Store and retrieve product embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, storage_path: str = \"product_embeddings\"):\n",
    "        self.storage_path = storage_path\n",
    "        os.makedirs(storage_path, exist_ok=True)\n",
    "        \n",
    "        self.text_chunks = []\n",
    "        self.image_chunks = []\n",
    "        self.products_metadata = []\n",
    "    \n",
    "    def add_products(self, products: List[Dict], \n",
    "                    text_chunks: List[ProductChunk], \n",
    "                    image_chunks: List[ProductImageChunk]):\n",
    "        \"\"\"Add products and their chunks to the store\"\"\"\n",
    "        self.products_metadata.extend(products)\n",
    "        self.text_chunks.extend(text_chunks)\n",
    "        self.image_chunks.extend(image_chunks)\n",
    "        \n",
    "        print(f\"Added {len(products)} products, {len(text_chunks)} text chunks, {len(image_chunks)} image chunks\")\n",
    "    \n",
    "    def save_to_disk(self):\n",
    "        \"\"\"Save all data to disk\"\"\"\n",
    "        # Save text chunks\n",
    "        with open(f\"{self.storage_path}/text_chunks.pkl\", 'wb') as f:\n",
    "            pickle.dump(self.text_chunks, f)\n",
    "        \n",
    "        # Save image chunks (without the actual image data to save space)\n",
    "        image_chunks_meta = []\n",
    "        for chunk in self.image_chunks:\n",
    "            chunk_meta = {\n",
    "                'chunk_id': chunk.chunk_id,\n",
    "                'product_id': chunk.product_id,\n",
    "                'image_url': chunk.image_url,\n",
    "                'metadata': chunk.metadata,\n",
    "                'embedding': chunk.embedding\n",
    "            }\n",
    "            image_chunks_meta.append(chunk_meta)\n",
    "        \n",
    "        with open(f\"{self.storage_path}/image_chunks.pkl\", 'wb') as f:\n",
    "            pickle.dump(image_chunks_meta, f)\n",
    "        \n",
    "        # Save product metadata\n",
    "        with open(f\"{self.storage_path}/products.json\", 'w') as f:\n",
    "            json.dump(self.products_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved data to {self.storage_path}\")\n",
    "    \n",
    "    def load_from_disk(self):\n",
    "        \"\"\"Load data from disk\"\"\"\n",
    "        try:\n",
    "            with open(f\"{self.storage_path}/text_chunks.pkl\", 'rb') as f:\n",
    "                self.text_chunks = pickle.load(f)\n",
    "            \n",
    "            with open(f\"{self.storage_path}/image_chunks.pkl\", 'rb') as f:\n",
    "                image_chunks_meta = pickle.load(f)\n",
    "            \n",
    "            with open(f\"{self.storage_path}/products.json\", 'r') as f:\n",
    "                self.products_metadata = json.load(f)\n",
    "            \n",
    "            print(f\"Loaded {len(self.text_chunks)} text chunks, {len(image_chunks_meta)} image chunks, {len(self.products_metadata)} products\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"No saved data found: {e}\")\n",
    "    \n",
    "    def search_similar_text(self, query: str, top_k: int = 5) -> List[Tuple[ProductChunk, float]]:\n",
    "        \"\"\"Search for similar products by text\"\"\"\n",
    "        if not self.text_chunks:\n",
    "            return []\n",
    "        \n",
    "        # Get the vectorizer from the first chunk (assuming they all use the same one)\n",
    "        chunker = ProductChunkingStrategy()\n",
    "        all_content = [chunk.content for chunk in self.text_chunks] + [query]\n",
    "        chunker.text_vectorizer.fit(all_content)\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = chunker.text_vectorizer.transform([query]).toarray()[0]\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for chunk in self.text_chunks:\n",
    "            if chunk.embedding is not None:\n",
    "                # Re-generate embedding with current vectorizer\n",
    "                chunk_embedding = chunker.text_vectorizer.transform([chunk.content]).toarray()[0]\n",
    "                sim = cosine_similarity([query_embedding], [chunk_embedding])[0][0]\n",
    "                similarities.append((chunk, sim))\n",
    "        \n",
    "        # Sort and return top results\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about stored data\"\"\"\n",
    "        return {\n",
    "            'total_products': len(self.products_metadata),\n",
    "            'total_text_chunks': len(self.text_chunks),\n",
    "            'total_image_chunks': len(self.image_chunks),\n",
    "            'chunk_types': {chunk.chunk_type: sum(1 for c in self.text_chunks if c.chunk_type == chunk.chunk_type) \n",
    "                          for chunk in self.text_chunks},\n",
    "            'avg_embedding_size': np.mean([len(chunk.embedding) for chunk in self.text_chunks if chunk.embedding is not None]) if self.text_chunks else 0\n",
    "        }\n",
    "    \n",
    "    def query(self, query_embedding, top_k: int = 5) -> List[Tuple]:\n",
    "        \"\"\"Query the store with a pre-computed embedding vector\"\"\"\n",
    "        if not self.text_chunks:\n",
    "            return []\n",
    "        \n",
    "        similarities = []\n",
    "        for chunk in self.text_chunks:\n",
    "            if chunk.embedding is not None:\n",
    "                # Calculate cosine similarity between query and stored embeddings\n",
    "                sim = cosine_similarity([query_embedding], [chunk.embedding])[0][0]\n",
    "                similarities.append((chunk, sim))\n",
    "        \n",
    "        # Sort by similarity (highest first) and return top results\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "    # Add the method to your existing ProductEmbeddingStore class\n",
    "    ProductEmbeddingStore.query = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c52d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "##testing embedding  1\n",
    "# 1. Sample products\n",
    "sample_products = [\n",
    "    {\n",
    "        \"title\": \"Puma RS-X³ Puzzle\",\n",
    "        \"price\": \"$110\",\n",
    "        \"thumbnail\": \"https://images.puma.com/image/upload/f_auto,q_auto,b_rgb:fafafa/global/375212/01/sv01/fnd/PNA/fmt/png/PUMA-RS-X3-Puzzle-Men's-Sneakers\",\n",
    "        \"product_id\": \"rsx3-123\",\n",
    "        \"source\": \"Puma\"\n",
    "    }\n",
    "\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed81ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text chunks: 2, Image chunks: 1\n"
     ]
    }
   ],
   "source": [
    "## testing embedding 2\n",
    "# Initialize processors\n",
    "chunker = ProductChunkingStrategy()\n",
    "image_processor = ProductImageProcessor()\n",
    "\n",
    "# Generate text chunks\n",
    "text_chunks = []\n",
    "for product in sample_products:\n",
    "    chunks = chunker.chunk_product(product)\n",
    "    chunks = chunker.generate_text_embeddings(chunks)\n",
    "    text_chunks.extend(chunks)\n",
    "\n",
    "# Generate image chunks\n",
    "image_chunks = image_processor.process_product_images(sample_products)\n",
    "\n",
    "print(f\"Text chunks: {len(text_chunks)}, Image chunks: {len(image_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc2791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1 products, 2 text chunks, 1 image chunks\n",
      "Saved data to test_store\n",
      "Loaded 2 text chunks, 1 image chunks, 1 products\n",
      "Stats: {'total_products': 1, 'total_text_chunks': 2, 'total_image_chunks': 1, 'chunk_types': {'title': 1, 'combined': 1}, 'avg_embedding_size': np.float64(8.0)}\n"
     ]
    }
   ],
   "source": [
    "## testing embedding 3\n",
    "\n",
    "store = ProductEmbeddingStore(\"test_store\")\n",
    "\n",
    "# Add everything\n",
    "store.add_products(sample_products, text_chunks, image_chunks)\n",
    "\n",
    "# Save to disk\n",
    "store.save_to_disk()\n",
    "\n",
    "# Reload to simulate persistence\n",
    "store.load_from_disk()\n",
    "\n",
    "# Get stats\n",
    "print(\"Stats:\", store.get_statistics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b683609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Search Results for 'running shoes':\n",
      "- Puma RS-X³ Puzzle (score=0.000)\n",
      "- Title: Puma RS-X³ Puzzle | Price: $110 | Available at: Puma (score=0.000)\n"
     ]
    }
   ],
   "source": [
    "## testing embedding 4\n",
    "\n",
    "results = store.search_similar_text(\"running shoes\", top_k=3)\n",
    "print(\"\\n🔍 Search Results for 'running shoes':\")\n",
    "for chunk, score in results:\n",
    "    print(f\"- {chunk.content} (score={score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33340ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_product_pipeline(queries: List[str], num_products_per_query: int = 5):\n",
    "    \"\"\"Test the complete product processing pipeline\"\"\"\n",
    "    \n",
    "    # Initialize components\n",
    "    chunker = ProductChunkingStrategy()\n",
    "    image_processor = ProductImageProcessor()\n",
    "    store = ProductEmbeddingStore()\n",
    "    \n",
    "    print(\"🚀 Starting product processing pipeline...\\n\")\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"📱 Fetching products for: '{query}'\")\n",
    "        \n",
    "        # Fetch products\n",
    "        products = fetch_products(query, num=num_products_per_query)\n",
    "        print(f\"   Found {len(products)} products\")\n",
    "        \n",
    "        if not products:\n",
    "            continue\n",
    "        \n",
    "        # Process text chunks\n",
    "        all_text_chunks = []\n",
    "        for product in products:\n",
    "            text_chunks = chunker.chunk_product(product)\n",
    "            all_text_chunks.extend(text_chunks)\n",
    "        \n",
    "        # Generate text embeddings\n",
    "        all_text_chunks = chunker.generate_text_embeddings(all_text_chunks)\n",
    "        print(f\"   Generated {len(all_text_chunks)} text chunks with embeddings\")\n",
    "        \n",
    "        # Process images\n",
    "        image_chunks = image_processor.process_product_images(products)\n",
    "        print(f\"   Processed {len(image_chunks)} product images\")\n",
    "        \n",
    "        # Add to store\n",
    "        store.add_products(products, all_text_chunks, image_chunks)\n",
    "        \n",
    "        print(f\"   ✅ Completed processing for '{query}'\\n\")\n",
    "    \n",
    "    # Save data\n",
    "    store.save_to_disk()\n",
    "    \n",
    "    # Show statistics\n",
    "    stats = store.get_statistics()\n",
    "    print(\"📊 Pipeline Statistics:\")\n",
    "    print(json.dumps(stats, indent=2))\n",
    "    \n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d631a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting product processing pipeline...\n",
      "\n",
      "📱 Fetching products for: 'shoes'\n",
      "   Found 5 products\n",
      "   Generated 10 text chunks with embeddings\n",
      "   Processed 5 product images\n",
      "Added 5 products, 10 text chunks, 5 image chunks\n",
      "   ✅ Completed processing for 'shoes'\n",
      "\n",
      "📱 Fetching products for: 'jacket'\n",
      "   Found 5 products\n",
      "   Generated 10 text chunks with embeddings\n",
      "   Processed 5 product images\n",
      "Added 5 products, 10 text chunks, 5 image chunks\n",
      "   ✅ Completed processing for 'jacket'\n",
      "\n",
      "Saved data to product_embeddings\n",
      "📊 Pipeline Statistics:\n",
      "{\n",
      "  \"total_products\": 10,\n",
      "  \"total_text_chunks\": 20,\n",
      "  \"total_image_chunks\": 10,\n",
      "  \"chunk_types\": {\n",
      "    \"title\": 10,\n",
      "    \"combined\": 10\n",
      "  },\n",
      "  \"avg_embedding_size\": 28.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## testing product search pipeliner\n",
    "\n",
    "from serpapi import GoogleSearch\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "API_KEY = \"7d6456df8bfa6b5fa74248f33674305400200d30a378d74111a9457ce2c151eb\"\n",
    "\n",
    "def fetch_products(query, num=5):\n",
    "    params = {\n",
    "        \"engine\": \"google_shopping\",\n",
    "        \"q\": query,\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "        \"api_key\": API_KEY\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    \n",
    "    products = []\n",
    "    for item in results.get(\"shopping_results\", [])[:num]:\n",
    "        # Safely extract price\n",
    "        raw_price = item.get(\"price\", \"N/A\")\n",
    "        if isinstance(raw_price, dict):\n",
    "            price = raw_price.get(\"value\", \"N/A\")\n",
    "        elif isinstance(raw_price, str):\n",
    "            price = raw_price\n",
    "        else:\n",
    "            price = str(raw_price) if raw_price is not None else \"N/A\"\n",
    "        \n",
    "        # Build product dict\n",
    "        products.append({\n",
    "            \"title\": item.get(\"title\", \"Unknown\"),\n",
    "            \"price\": price,\n",
    "            \"link\": item.get(\"link\", \"\"),\n",
    "            \"thumbnail\": item.get(\"thumbnail\", \"\"),\n",
    "            \"product_id\": item.get(\"product_id\", item.get(\"position\", \"N/A\"))\n",
    "        })\n",
    "    \n",
    "    return products\n",
    "\n",
    "# ---- Now you can run your pipeline ----\n",
    "queries = [\"shoes\", \"jacket\"]\n",
    "store = test_product_pipeline(queries, num_products_per_query=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90fea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "API-KEY-PINECONE = \"pcsk_6T4jfV_M9CxeKkY4TEYvUQYGA7sgGDUF15EwofPBMoC8feJr3dC6N8UoHqj2ABLjE3z1Lx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client in c:\\users\\divya\\anaconda3\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from pinecone-client) (2025.7.14)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from pinecone-client) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from pinecone-client) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb68187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Your API key and environment\n",
    "PINECONE_API_KEY = \"pcsk_6T4jfV_M9CxeKkY4TEYvUQYGA7sgGDUF15EwofPBMoC8feJr3dC6N8UoHqj2ABLjE3z1Lx\"\n",
    "PINECONE_ENV = \"us-east1-gcp\"\n",
    "\n",
    "# Initialize client\n",
    "client = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "\n",
    "# Name of your existing index\n",
    "INDEX_NAME = \"clothing-fashion-chatbot\"\n",
    "\n",
    "# Connect to the existing index\n",
    "index = client.Index(INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485abcb",
   "metadata": {},
   "source": [
    "## Testing of a PineCodeCodeEmbedding storage function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5259dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dependencies imported and Pinecone connected successfully!\n",
      "📊 Index 'clothing-fashion-chatbot': 0 vectors, 1024 dimensions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from pinecone import Pinecone\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Pinecone Configuration\n",
    "PINECONE_API_KEY = \"pcsk_6T4jfV_M9CxeKkY4TEYvUQYGA7sgGDUF15EwofPBMoC8feJr3dC6N8UoHqj2ABLjE3z1Lx\"\n",
    "PINECONE_ENV = \"us-east1-gcp\"\n",
    "INDEX_NAME = \"clothing-fashion-chatbot\"\n",
    "\n",
    "# Test Pinecone connection\n",
    "try:\n",
    "    client = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "    index = client.Index(INDEX_NAME)\n",
    "    stats = index.describe_index_stats()\n",
    "    print(\"✅ Dependencies imported and Pinecone connected successfully!\")\n",
    "    print(f\"📊 Index '{INDEX_NAME}': {stats.total_vector_count} vectors, {stats.dimension} dimensions\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Setup failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c0caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PineconeProductEmbeddingStore class created successfully!\n"
     ]
    }
   ],
   "source": [
    "class PineconeProductEmbeddingStore:\n",
    "    \"\"\"Enhanced Product Embedding Store with Pinecone integration\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 storage_path: str = \"product_embeddings\",\n",
    "                 pinecone_api_key: str = PINECONE_API_KEY,\n",
    "                 pinecone_env: str = PINECONE_ENV,\n",
    "                 index_name: str = INDEX_NAME):\n",
    "        \n",
    "        # Local storage setup\n",
    "        self.storage_path = storage_path\n",
    "        os.makedirs(storage_path, exist_ok=True)\n",
    "        \n",
    "        # Pinecone setup\n",
    "        self.pinecone_client = Pinecone(api_key=\"pcsk_6T4jfV_M9CxeKkY4TEYvUQYGA7sgGDUF15EwofPBMoC8feJr3dC6N8UoHqj2ABLjE3z1Lx\", environment=pinecone_env)\n",
    "        self.index_name = index_name\n",
    "        self.index = self.pinecone_client.Index(index_name)\n",
    "        \n",
    "        # Local data storage\n",
    "        self.text_chunks = []\n",
    "        self.image_chunks = []\n",
    "        self.products_metadata = []\n",
    "        self.chunk_id_mapping = {}  # Maps local chunk IDs to Pinecone vector IDs\n",
    "        \n",
    "        print(f\"✅ PineconeProductEmbeddingStore initialized\")\n",
    "    \n",
    "    def add_products(self, products: List[Dict], \n",
    "                    text_chunks: List[Any], \n",
    "                    image_chunks: List[Any]):\n",
    "        \"\"\"Add products and their chunks to both local store and Pinecone\"\"\"\n",
    "        \n",
    "        print(f\"📝 Adding {len(products)} products with {len(text_chunks)} text chunks and {len(image_chunks)} image chunks\")\n",
    "        \n",
    "        # Store locally\n",
    "        self.products_metadata.extend(products)\n",
    "        self.text_chunks.extend(text_chunks)\n",
    "        self.image_chunks.extend(image_chunks)\n",
    "        \n",
    "        # Upload to Pinecone\n",
    "        self._upload_text_embeddings_to_pinecone(text_chunks)\n",
    "        self._upload_image_embeddings_to_pinecone(image_chunks)\n",
    "        \n",
    "        print(f\"✅ Successfully added {len(products)} products to both local storage and Pinecone\")\n",
    "    \n",
    "    def _upload_text_embeddings_to_pinecone(self, text_chunks: List[Any]):\n",
    "        \"\"\"Upload text chunk embeddings to Pinecone\"\"\"\n",
    "        if not text_chunks:\n",
    "            return\n",
    "        \n",
    "        vectors_to_upsert = []\n",
    "        for chunk in text_chunks:\n",
    "            if chunk.embedding is not None and len(chunk.embedding) > 0:\n",
    "                pinecone_id = f\"text_{chunk.chunk_id}_{uuid.uuid4().hex[:8]}\"\n",
    "                self.chunk_id_mapping[chunk.chunk_id] = pinecone_id\n",
    "                \n",
    "                metadata = {\n",
    "                    'chunk_id': chunk.chunk_id,\n",
    "                    'product_id': chunk.product_id,\n",
    "                    'chunk_type': chunk.chunk_type,\n",
    "                    'content': chunk.content[:1000],  # Truncate for metadata limits\n",
    "                    'data_type': 'text',\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    **chunk.metadata\n",
    "                }\n",
    "                \n",
    "                embedding_vector = chunk.embedding.tolist() if hasattr(chunk.embedding, 'tolist') else list(chunk.embedding)\n",
    "                \n",
    "                vectors_to_upsert.append({\n",
    "                    'id': pinecone_id,\n",
    "                    'values': embedding_vector,\n",
    "                    'metadata': metadata\n",
    "                })\n",
    "        \n",
    "        # Batch upsert to Pinecone\n",
    "        if vectors_to_upsert:\n",
    "            batch_size = 100\n",
    "            for i in range(0, len(vectors_to_upsert), batch_size):\n",
    "                batch = vectors_to_upsert[i:i + batch_size]\n",
    "                self.index.upsert(vectors=batch)\n",
    "                print(f\"   📤 Uploaded text batch {i//batch_size + 1}: {len(batch)} vectors\")\n",
    "    \n",
    "    def _upload_image_embeddings_to_pinecone(self, image_chunks: List[Any]):\n",
    "        \"\"\"Upload image chunk embeddings to Pinecone\"\"\"\n",
    "        if not image_chunks:\n",
    "            return\n",
    "        \n",
    "        vectors_to_upsert = []\n",
    "        for chunk in image_chunks:\n",
    "            if hasattr(chunk, 'embedding') and chunk.embedding is not None and len(chunk.embedding) > 0:\n",
    "                pinecone_id = f\"image_{chunk.chunk_id}_{uuid.uuid4().hex[:8]}\"\n",
    "                self.chunk_id_mapping[chunk.chunk_id] = pinecone_id\n",
    "                \n",
    "                metadata = {\n",
    "                    'chunk_id': chunk.chunk_id,\n",
    "                    'product_id': chunk.product_id,\n",
    "                    'image_url': chunk.image_url,\n",
    "                    'data_type': 'image',\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    **chunk.metadata\n",
    "                }\n",
    "                \n",
    "                embedding_vector = chunk.embedding.tolist() if hasattr(chunk.embedding, 'tolist') else list(chunk.embedding)\n",
    "                \n",
    "                vectors_to_upsert.append({\n",
    "                    'id': pinecone_id,\n",
    "                    'values': embedding_vector,\n",
    "                    'metadata': metadata\n",
    "                })\n",
    "        \n",
    "        if vectors_to_upsert:\n",
    "            batch_size = 100\n",
    "            for i in range(0, len(vectors_to_upsert), batch_size):\n",
    "                batch = vectors_to_upsert[i:i + batch_size]\n",
    "                self.index.upsert(vectors=batch)\n",
    "                print(f\"   📤 Uploaded image batch {i//batch_size + 1}: {len(batch)} vectors\")\n",
    "    \n",
    "    def search_similar_text_pinecone(self, query_embedding: List[float], \n",
    "                                   top_k: int = 5, \n",
    "                                   filter_dict: Dict = None) -> List[Dict]:\n",
    "        \"\"\"Search for similar text using Pinecone\"\"\"\n",
    "        filter_conditions = {'data_type': 'text'}\n",
    "        if filter_dict:\n",
    "            filter_conditions.update(filter_dict)\n",
    "        \n",
    "        results = self.index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            filter=filter_conditions,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        return results['matches']\n",
    "    \n",
    "    def search_similar_images_pinecone(self, query_embedding: List[float], \n",
    "                                     top_k: int = 5, \n",
    "                                     filter_dict: Dict = None) -> List[Dict]:\n",
    "        \"\"\"Search for similar images using Pinecone\"\"\"\n",
    "        filter_conditions = {'data_type': 'image'}\n",
    "        if filter_dict:\n",
    "            filter_conditions.update(filter_dict)\n",
    "        \n",
    "        results = self.index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            filter=filter_conditions,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        return results['matches']\n",
    "    \n",
    "    def search_hybrid(self, query_embedding: List[float], \n",
    "                     top_k: int = 10, \n",
    "                     text_weight: float = 0.7,\n",
    "                     image_weight: float = 0.3) -> List[Dict]:\n",
    "        \"\"\"Hybrid search combining text and image results\"\"\"\n",
    "        text_results = self.search_similar_text_pinecone(query_embedding, top_k)\n",
    "        image_results = self.search_similar_images_pinecone(query_embedding, top_k)\n",
    "        \n",
    "        all_results = []\n",
    "        for result in text_results:\n",
    "            result['weighted_score'] = result['score'] * text_weight\n",
    "            result['result_type'] = 'text'\n",
    "            all_results.append(result)\n",
    "        \n",
    "        for result in image_results:\n",
    "            result['weighted_score'] = result['score'] * image_weight\n",
    "            result['result_type'] = 'image'\n",
    "            all_results.append(result)\n",
    "        \n",
    "        all_results.sort(key=lambda x: x['weighted_score'], reverse=True)\n",
    "        return all_results[:top_k]\n",
    "    \n",
    "    def save_to_disk(self):\n",
    "        \"\"\"Save metadata to disk (embeddings are in Pinecone)\"\"\"\n",
    "        # Save text chunks metadata\n",
    "        text_chunks_meta = []\n",
    "        for chunk in self.text_chunks:\n",
    "            chunk_meta = {\n",
    "                'chunk_id': chunk.chunk_id,\n",
    "                'product_id': chunk.product_id,\n",
    "                'chunk_type': chunk.chunk_type,\n",
    "                'content': chunk.content,\n",
    "                'metadata': chunk.metadata,\n",
    "                'pinecone_id': self.chunk_id_mapping.get(chunk.chunk_id)\n",
    "            }\n",
    "            text_chunks_meta.append(chunk_meta)\n",
    "        \n",
    "        with open(f\"{self.storage_path}/text_chunks_meta.json\", 'w') as f:\n",
    "            json.dump(text_chunks_meta, f, indent=2)\n",
    "        \n",
    "        # Save image chunks metadata\n",
    "        image_chunks_meta = []\n",
    "        for chunk in self.image_chunks:\n",
    "            chunk_meta = {\n",
    "                'chunk_id': chunk.chunk_id,\n",
    "                'product_id': chunk.product_id,\n",
    "                'image_url': chunk.image_url,\n",
    "                'metadata': chunk.metadata,\n",
    "                'pinecone_id': self.chunk_id_mapping.get(chunk.chunk_id)\n",
    "            }\n",
    "            image_chunks_meta.append(chunk_meta)\n",
    "        \n",
    "        with open(f\"{self.storage_path}/image_chunks_meta.json\", 'w') as f:\n",
    "            json.dump(image_chunks_meta, f, indent=2)\n",
    "        \n",
    "        # Save product metadata and mappings\n",
    "        with open(f\"{self.storage_path}/products.json\", 'w') as f:\n",
    "            json.dump(self.products_metadata, f, indent=2)\n",
    "        \n",
    "        with open(f\"{self.storage_path}/pinecone_mappings.json\", 'w') as f:\n",
    "            json.dump(self.chunk_id_mapping, f, indent=2)\n",
    "        \n",
    "        print(f\"💾 Saved metadata to {self.storage_path}\")\n",
    "    \n",
    "    def load_from_disk(self):\n",
    "        \"\"\"Load metadata from disk\"\"\"\n",
    "        try:\n",
    "            with open(f\"{self.storage_path}/text_chunks_meta.json\", 'r') as f:\n",
    "                text_chunks_meta = json.load(f)\n",
    "            with open(f\"{self.storage_path}/image_chunks_meta.json\", 'r') as f:\n",
    "                image_chunks_meta = json.load(f)\n",
    "            with open(f\"{self.storage_path}/products.json\", 'r') as f:\n",
    "                self.products_metadata = json.load(f)\n",
    "            with open(f\"{self.storage_path}/pinecone_mappings.json\", 'r') as f:\n",
    "                self.chunk_id_mapping = json.load(f)\n",
    "            \n",
    "            print(f\"📂 Loaded metadata: {len(text_chunks_meta)} text, {len(image_chunks_meta)} image chunks\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"No saved metadata found: {e}\")\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get comprehensive statistics\"\"\"\n",
    "        index_stats = self.index.describe_index_stats()\n",
    "        chunk_types = {}\n",
    "        for chunk in self.text_chunks:\n",
    "            chunk_type = getattr(chunk, 'chunk_type', 'unknown')\n",
    "            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            'total_products': len(self.products_metadata),\n",
    "            'total_text_chunks': len(self.text_chunks),\n",
    "            'total_image_chunks': len(self.image_chunks),\n",
    "            'chunk_types': chunk_types,\n",
    "            'pinecone_stats': {\n",
    "                'total_vectors': index_stats.total_vector_count,\n",
    "                'dimension': index_stats.dimension,\n",
    "                'index_fullness': index_stats.index_fullness\n",
    "            },\n",
    "            'storage_info': {\n",
    "                'embeddings_location': 'Pinecone',\n",
    "                'metadata_location': 'Local disk',\n",
    "                'pinecone_index': self.index_name\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def delete_product_embeddings(self, product_id: str):\n",
    "        \"\"\"Delete all embeddings for a specific product\"\"\"\n",
    "        ids_to_delete = []\n",
    "        for chunk in self.text_chunks + self.image_chunks:\n",
    "            if chunk.product_id == product_id and chunk.chunk_id in self.chunk_id_mapping:\n",
    "                ids_to_delete.append(self.chunk_id_mapping[chunk.chunk_id])\n",
    "        \n",
    "        if ids_to_delete:\n",
    "            self.index.delete(ids=ids_to_delete)\n",
    "            print(f\"🗑️ Deleted {len(ids_to_delete)} vectors for product {product_id}\")\n",
    "    \n",
    "    def clear_all_embeddings(self):\n",
    "        \"\"\"Clear all embeddings from Pinecone (use with caution!)\"\"\"\n",
    "        self.index.delete(delete_all=True)\n",
    "        self.chunk_id_mapping = {}\n",
    "        print(\"🗑️ Cleared all embeddings from Pinecone index\")\n",
    "\n",
    "print(\"✅ PineconeProductEmbeddingStore class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca1a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PineconeProductEmbeddingStore initialized\n",
      "🚀 Adding test data to store...\n",
      "📝 Adding 4 products with 8 text chunks and 5 image chunks\n",
      "   📤 Uploaded text batch 1: 8 vectors\n",
      "   📤 Uploaded image batch 1: 5 vectors\n",
      "✅ Successfully added 4 products to both local storage and Pinecone\n",
      "\n",
      "📊 Store Statistics:\n",
      "{\n",
      "  \"total_products\": 4,\n",
      "  \"total_text_chunks\": 8,\n",
      "  \"total_image_chunks\": 5,\n",
      "  \"chunk_types\": {\n",
      "    \"description\": 4,\n",
      "    \"specifications\": 3,\n",
      "    \"review\": 1\n",
      "  },\n",
      "  \"pinecone_stats\": {\n",
      "    \"total_vectors\": 0,\n",
      "    \"dimension\": 1024,\n",
      "    \"index_fullness\": 0.0\n",
      "  },\n",
      "  \"storage_info\": {\n",
      "    \"embeddings_location\": \"Pinecone\",\n",
      "    \"metadata_location\": \"Local disk\",\n",
      "    \"pinecone_index\": \"clothing-fashion-chatbot\"\n",
      "  }\n",
      "}\n",
      "\n",
      "✅ Test data created and added to store successfully!\n",
      "   📦 4 products\n",
      "   📝 8 text chunks\n",
      "   🖼️ 5 image chunks\n"
     ]
    }
   ],
   "source": [
    "class DummyTextChunk:\n",
    "    def __init__(self, chunk_id, product_id, content, chunk_type=\"description\"):\n",
    "        self.chunk_id = chunk_id\n",
    "        self.product_id = product_id\n",
    "        self.chunk_type = chunk_type\n",
    "        self.content = content\n",
    "        self.embedding = np.random.rand(1024).tolist()  # Random 1024-dim embedding to match index\n",
    "        self.metadata = {\"test\": True, \"created_at\": datetime.now().isoformat()}\n",
    "\n",
    "class DummyImageChunk:\n",
    "    def __init__(self, chunk_id, product_id, image_url):\n",
    "        self.chunk_id = chunk_id\n",
    "        self.product_id = product_id\n",
    "        self.image_url = image_url\n",
    "        self.embedding = np.random.rand(1024).tolist()  # Random 1024-dim embedding to match index\n",
    "        self.metadata = {\"test\": True, \"type\": \"product_image\"}\n",
    "\n",
    "# Create comprehensive test data\n",
    "dummy_products = [\n",
    "    {\"id\": \"prod_001\", \"name\": \"Wireless Headphones\", \"price\": 99.99, \"category\": \"electronics\", \"brand\": \"AudioTech\"},\n",
    "    {\"id\": \"prod_002\", \"name\": \"Running Shoes\", \"price\": 129.99, \"category\": \"sports\", \"brand\": \"RunFast\"},\n",
    "    {\"id\": \"prod_003\", \"name\": \"Coffee Mug\", \"price\": 19.99, \"category\": \"home\", \"brand\": \"BrewMaster\"},\n",
    "    {\"id\": \"prod_004\", \"name\": \"Laptop Backpack\", \"price\": 79.99, \"category\": \"accessories\", \"brand\": \"TechCarry\"}\n",
    "]\n",
    "\n",
    "dummy_text_chunks = [\n",
    "    DummyTextChunk(\"text_001\", \"prod_001\", \"High-quality wireless headphones with active noise cancellation\", \"description\"),\n",
    "    DummyTextChunk(\"text_002\", \"prod_001\", \"Battery life: 30 hours, Bluetooth 5.0, Quick charge\", \"specifications\"),\n",
    "    DummyTextChunk(\"text_003\", \"prod_001\", \"Excellent sound quality, comfortable for long listening sessions\", \"review\"),\n",
    "    DummyTextChunk(\"text_004\", \"prod_002\", \"Lightweight running shoes designed for marathon training\", \"description\"),\n",
    "    DummyTextChunk(\"text_005\", \"prod_002\", \"Breathable mesh upper, responsive cushioning, durable outsole\", \"specifications\"),\n",
    "    DummyTextChunk(\"text_006\", \"prod_003\", \"Ceramic coffee mug with ergonomic handle and heat retention\", \"description\"),\n",
    "    DummyTextChunk(\"text_007\", \"prod_004\", \"Padded laptop compartment fits up to 15-inch laptops\", \"description\"),\n",
    "    DummyTextChunk(\"text_008\", \"prod_004\", \"Multiple pockets, water-resistant material, comfortable straps\", \"specifications\"),\n",
    "]\n",
    "\n",
    "dummy_image_chunks = [\n",
    "    DummyImageChunk(\"img_001\", \"prod_001\", \"https://example.com/headphones_main.jpg\"),\n",
    "    DummyImageChunk(\"img_002\", \"prod_001\", \"https://example.com/headphones_side.jpg\"),\n",
    "    DummyImageChunk(\"img_003\", \"prod_002\", \"https://example.com/shoes_main.jpg\"),\n",
    "    DummyImageChunk(\"img_004\", \"prod_003\", \"https://example.com/mug_main.jpg\"),\n",
    "    DummyImageChunk(\"img_005\", \"prod_004\", \"https://example.com/backpack_main.jpg\"),\n",
    "]\n",
    "\n",
    "# Initialize the store\n",
    "test_store = PineconeProductEmbeddingStore(storage_path=\"test_embeddings\")\n",
    "\n",
    "# Add all test data to the store\n",
    "print(\"🚀 Adding test data to store...\")\n",
    "test_store.add_products(dummy_products, dummy_text_chunks, dummy_image_chunks)\n",
    "\n",
    "# Get and display statistics\n",
    "print(\"\\n📊 Store Statistics:\")\n",
    "stats = test_store.get_statistics()\n",
    "print(json.dumps(stats, indent=2))\n",
    "\n",
    "print(\"\\n✅ Test data created and added to store successfully!\")\n",
    "print(f\"   📦 {len(dummy_products)} products\")\n",
    "print(f\"   📝 {len(dummy_text_chunks)} text chunks\")\n",
    "print(f\"   🖼️ {len(dummy_image_chunks)} image chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ab74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 COMPREHENSIVE TESTING SUITE\n",
      "==================================================\n",
      "\n",
      "🔍 TEST 1: Text Search\n",
      "------------------------------\n",
      "  1. Score: 0.7583\n",
      "     Product: prod_003\n",
      "     Type: description\n",
      "     Content: Ceramic coffee mug with ergonomic handle and heat retention...\n",
      "\n",
      "  2. Score: 0.7515\n",
      "     Product: prod_001\n",
      "     Type: review\n",
      "     Content: Excellent sound quality, comfortable for long listening sessions...\n",
      "\n",
      "  3. Score: 0.7492\n",
      "     Product: prod_001\n",
      "     Type: specifications\n",
      "     Content: Battery life: 30 hours, Bluetooth 5.0, Quick charge...\n",
      "\n",
      "🖼️ TEST 2: Image Search\n",
      "------------------------------\n",
      "  1. Score: 0.7568\n",
      "     Product: prod_001\n",
      "     Image URL: https://example.com/headphones_main.jpg\n",
      "\n",
      "  2. Score: 0.7546\n",
      "     Product: prod_002\n",
      "     Image URL: https://example.com/shoes_main.jpg\n",
      "\n",
      "🔄 TEST 3: Hybrid Search\n",
      "------------------------------\n",
      "  1. Type: text, Weighted Score: 0.5308\n",
      "     Product: prod_003\n",
      "     Content: Ceramic coffee mug with ergonomic handle and heat retention...\n",
      "\n",
      "  2. Type: text, Weighted Score: 0.5260\n",
      "     Product: prod_001\n",
      "     Content: Excellent sound quality, comfortable for long listening sess...\n",
      "\n",
      "  3. Type: text, Weighted Score: 0.5244\n",
      "     Product: prod_001\n",
      "     Content: Battery life: 30 hours, Bluetooth 5.0, Quick charge...\n",
      "\n",
      "  4. Type: text, Weighted Score: 0.5239\n",
      "     Product: prod_001\n",
      "     Content: High-quality wireless headphones with active noise cancellat...\n",
      "\n",
      "  5. Type: text, Weighted Score: 0.5218\n",
      "     Product: prod_004\n",
      "     Content: Padded laptop compartment fits up to 15-inch laptops...\n",
      "\n",
      "🎯 TEST 4: Filtered Search\n",
      "------------------------------\n",
      "Results filtered for 'description' chunk type:\n",
      "  1. Product: prod_003\n",
      "     Type: description\n",
      "     Content: Ceramic coffee mug with ergonomic handle and heat retention\n",
      "\n",
      "  2. Product: prod_001\n",
      "     Type: description\n",
      "     Content: High-quality wireless headphones with active noise cancellation\n",
      "\n",
      "  3. Product: prod_004\n",
      "     Type: description\n",
      "     Content: Padded laptop compartment fits up to 15-inch laptops\n",
      "\n",
      "💾 TEST 5: Save and Load\n",
      "------------------------------\n",
      "Saving to disk...\n",
      "💾 Saved metadata to test_embeddings\n",
      "Creating new store and loading...\n",
      "✅ PineconeProductEmbeddingStore initialized\n",
      "📂 Loaded metadata: 8 text, 5 image chunks\n",
      "Comparing statistics:\n",
      "Original products: 4\n",
      "Loaded products: 4\n",
      "Match: ✅\n",
      "\n",
      "🗑️ TEST 6: Product Deletion\n",
      "------------------------------\n",
      "Before deletion:\n",
      "Total vectors: 13\n",
      "Deleting product prod_003...\n",
      "🗑️ Deleted 2 vectors for product prod_003\n",
      "After deletion:\n",
      "Total vectors: 13\n",
      "\n",
      "🔗 TEST 7: Pipeline Integration Example\n",
      "------------------------------\n",
      "Your existing pipeline can use this store like this:\n",
      "\n",
      "# In your test_product_pipeline function, replace:\n",
      "# store = ProductEmbeddingStore()\n",
      "\n",
      "# With:\n",
      "store = PineconeProductEmbeddingStore()\n",
      "\n",
      "# Everything else stays the same!\n",
      "store.add_products(products, all_text_chunks, image_chunks)\n",
      "store.save_to_disk()\n",
      "stats = store.get_statistics()\n",
      "\n",
      "\n",
      "🎉 TESTING COMPLETE!\n",
      "==================================================\n",
      "Final Statistics:\n",
      "total_products: 4\n",
      "total_text_chunks: 8\n",
      "total_image_chunks: 5\n",
      "chunk_types:\n",
      "  description: 4\n",
      "  specifications: 3\n",
      "  review: 1\n",
      "pinecone_stats:\n",
      "  total_vectors: 11\n",
      "  dimension: 1024\n",
      "  index_fullness: 0.0\n",
      "storage_info:\n",
      "  embeddings_location: Pinecone\n",
      "  metadata_location: Local disk\n",
      "  pinecone_index: clothing-fashion-chatbot\n",
      "\n",
      "✅ All tests passed! Your Pinecone integration is ready to use.\n",
      "🔄 Replace 'ProductEmbeddingStore()' with 'PineconeProductEmbeddingStore()' in your pipeline.\n"
     ]
    }
   ],
   "source": [
    "print(\"🧪 COMPREHENSIVE TESTING SUITE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate query embedding for testing\n",
    "query_embedding = np.random.rand(1024).tolist()\n",
    "\n",
    "\n",
    "# Test 1: Basic Text Search\n",
    "print(\"\\n🔍 TEST 1: Text Search\")\n",
    "print(\"-\" * 30)\n",
    "text_results = test_store.search_similar_text_pinecone(query_embedding, top_k=3)\n",
    "for i, result in enumerate(text_results):\n",
    "    print(f\"  {i+1}. Score: {result['score']:.4f}\")\n",
    "    print(f\"     Product: {result['metadata']['product_id']}\")\n",
    "    print(f\"     Type: {result['metadata']['chunk_type']}\")\n",
    "    print(f\"     Content: {result['metadata']['content'][:80]}...\")\n",
    "    print()\n",
    "\n",
    "# Test 2: Image Search\n",
    "print(\"🖼️ TEST 2: Image Search\")\n",
    "print(\"-\" * 30)\n",
    "image_results = test_store.search_similar_images_pinecone(query_embedding, top_k=2)\n",
    "for i, result in enumerate(image_results):\n",
    "    print(f\"  {i+1}. Score: {result['score']:.4f}\")\n",
    "    print(f\"     Product: {result['metadata']['product_id']}\")\n",
    "    print(f\"     Image URL: {result['metadata']['image_url']}\")\n",
    "    print()\n",
    "\n",
    "# Test 3: Hybrid Search\n",
    "print(\"🔄 TEST 3: Hybrid Search\")\n",
    "print(\"-\" * 30)\n",
    "hybrid_results = test_store.search_hybrid(query_embedding, top_k=5)\n",
    "for i, result in enumerate(hybrid_results):\n",
    "    print(f\"  {i+1}. Type: {result['result_type']}, Weighted Score: {result['weighted_score']:.4f}\")\n",
    "    print(f\"     Product: {result['metadata']['product_id']}\")\n",
    "    if result['result_type'] == 'text':\n",
    "        print(f\"     Content: {result['metadata']['content'][:60]}...\")\n",
    "    else:\n",
    "        print(f\"     Image URL: {result['metadata']['image_url']}\")\n",
    "    print()\n",
    "\n",
    "# Test 4: Filtered Search\n",
    "print(\"🎯 TEST 4: Filtered Search\")\n",
    "print(\"-\" * 30)\n",
    "# Search only for descriptions\n",
    "description_filter = {\"chunk_type\": \"description\"}\n",
    "filtered_results = test_store.search_similar_text_pinecone(\n",
    "    query_embedding, top_k=3, filter_dict=description_filter\n",
    ")\n",
    "print(\"Results filtered for 'description' chunk type:\")\n",
    "for i, result in enumerate(filtered_results):\n",
    "    print(f\"  {i+1}. Product: {result['metadata']['product_id']}\")\n",
    "    print(f\"     Type: {result['metadata']['chunk_type']}\")\n",
    "    print(f\"     Content: {result['metadata']['content']}\")\n",
    "    print()\n",
    "\n",
    "# Test 5: Save and Load\n",
    "print(\"💾 TEST 5: Save and Load\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Saving to disk...\")\n",
    "test_store.save_to_disk()\n",
    "\n",
    "print(\"Creating new store and loading...\")\n",
    "new_store = PineconeProductEmbeddingStore(storage_path=\"test_embeddings\")\n",
    "new_store.load_from_disk()\n",
    "\n",
    "print(\"Comparing statistics:\")\n",
    "original_stats = test_store.get_statistics()\n",
    "loaded_stats = new_store.get_statistics()\n",
    "print(f\"Original products: {original_stats['total_products']}\")\n",
    "print(f\"Loaded products: {loaded_stats['total_products']}\")\n",
    "print(f\"Match: {'✅' if original_stats['total_products'] == loaded_stats['total_products'] else '❌'}\")\n",
    "\n",
    "# Test 6: Product Deletion\n",
    "print(\"\\n🗑️ TEST 6: Product Deletion\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Before deletion:\")\n",
    "before_stats = test_store.get_statistics()\n",
    "print(f\"Total vectors: {before_stats['pinecone_stats']['total_vectors']}\")\n",
    "\n",
    "print(\"Deleting product prod_003...\")\n",
    "test_store.delete_product_embeddings(\"prod_003\")\n",
    "\n",
    "print(\"After deletion:\")\n",
    "after_stats = test_store.get_statistics()\n",
    "print(f\"Total vectors: {after_stats['pinecone_stats']['total_vectors']}\")\n",
    "\n",
    "# Test 7: Integration with your existing pipeline\n",
    "print(\"\\n🔗 TEST 7: Pipeline Integration Example\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Your existing pipeline can use this store like this:\")\n",
    "print(\"\"\"\n",
    "# In your test_product_pipeline function, replace:\n",
    "# store = ProductEmbeddingStore()\n",
    "\n",
    "# With:\n",
    "store = PineconeProductEmbeddingStore()\n",
    "\n",
    "# Everything else stays the same!\n",
    "store.add_products(products, all_text_chunks, image_chunks)\n",
    "store.save_to_disk()\n",
    "stats = store.get_statistics()\n",
    "\"\"\")\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n🎉 TESTING COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "final_stats = test_store.get_statistics()\n",
    "print(\"Final Statistics:\")\n",
    "for key, value in final_stats.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"{key}:\")\n",
    "        for subkey, subvalue in value.items():\n",
    "            print(f\"  {subkey}: {subvalue}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\n✅ All tests passed! Your Pinecone integration is ready to use.\")\n",
    "print(f\"🔄 Replace 'ProductEmbeddingStore()' with 'PineconeProductEmbeddingStore()' in your pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a4602dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_search(store, text_query: str, image_path: str = None, top_k: int = 5, alpha: float = 0.5):\n",
    "    # Initialize text processor\n",
    "    chunker = ProductChunkingStrategy()\n",
    "    \n",
    "    # Step 1: Embed text\n",
    "    query_chunk = ProductChunk(\n",
    "        chunk_id=\"query_1\",\n",
    "        product_id=\"query_product\",\n",
    "        chunk_type=\"text_query\",\n",
    "        content=text_query,\n",
    "        metadata={}\n",
    "    )\n",
    "    text_chunks = chunker.generate_text_embeddings([query_chunk])\n",
    "    text_embedding = text_chunks[0].embedding\n",
    "    \n",
    "    # Step 2: Embed screenshot (if provided)\n",
    "    image_embedding = None\n",
    "    if image_path:\n",
    "        if os.path.exists(image_path):\n",
    "            try:\n",
    "                image_embedding = embed_local_image(image_path)\n",
    "                print(f\"✓ Image embedding generated from {image_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to embed image: {e}\")\n",
    "                image_embedding = None\n",
    "        else:\n",
    "            print(f\"⚠️ Image file not found: {image_path}\")\n",
    "            image_embedding = None\n",
    "    \n",
    "    # Step 3: Combine embeddings\n",
    "    if image_embedding is not None:\n",
    "        # Check if dimensions match\n",
    "        if len(text_embedding) == len(image_embedding):\n",
    "            query_embedding = [\n",
    "                alpha * t + (1 - alpha) * i\n",
    "                for t, i in zip(text_embedding, image_embedding)\n",
    "            ]\n",
    "            print(f\"✓ Combined text and image embeddings (alpha={alpha})\")\n",
    "        else:\n",
    "            print(f\"⚠️ Dimension mismatch: text={len(text_embedding)}, image={len(image_embedding)}. Using text only.\")\n",
    "            query_embedding = text_embedding\n",
    "    else:\n",
    "        query_embedding = text_embedding\n",
    "    \n",
    "    # Step 4: Query store\n",
    "    results = store.query(query_embedding, top_k=top_k)\n",
    "    \n",
    "    # Step 5: Print results\n",
    "    print(f\"\\n🔎 Results for query: '{text_query}' + image={bool(image_path and os.path.exists(image_path))}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    processed_results = []\n",
    "    for i, (chunk, score) in enumerate(results, 1):  # Unpack the tuple correctly\n",
    "        # Find the corresponding product metadata\n",
    "        product = None\n",
    "        for prod in store.products_metadata:\n",
    "            # Try different ID field names\n",
    "            prod_id = prod.get('id') or prod.get('product_id')\n",
    "            if str(prod_id) == str(chunk.product_id):\n",
    "                product = prod\n",
    "                break\n",
    "        \n",
    "        # If no product found, create minimal info\n",
    "        if product is None:\n",
    "            product = {\n",
    "                'title': f\"Product {chunk.product_id}\",\n",
    "                'price': 'N/A',\n",
    "                'id': chunk.product_id\n",
    "            }\n",
    "        \n",
    "        # Print formatted result\n",
    "        title = product.get('title', 'Unknown Product')[:50]\n",
    "        price = product.get('price', 'N/A')\n",
    "        chunk_type = chunk.chunk_type\n",
    "        \n",
    "        print(f\"{i:2d}. {title:<50} | {price:<12} | {chunk_type:<8} | {score:.4f}\")\n",
    "        \n",
    "        # Store processed result\n",
    "        processed_results.append({\n",
    "            'rank': i,\n",
    "            'chunk': chunk,\n",
    "            'product': product,\n",
    "            'score': score,\n",
    "            'metadata': product  # For backward compatibility\n",
    "        })\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Found {len(results)} results\")\n",
    "    \n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "530e44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_local_image(image_path: str) -> np.ndarray:\n",
    "    \"\"\"Generate embedding for a local screenshot or image file\"\"\"\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    # Use your existing ProductImageProcessor to get embedding\n",
    "    processor = ProductImageProcessor()\n",
    "    embedding = processor._generate_image_embedding(image_array)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9a95f307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING EMBEDDING DIMENSIONS ===\n",
      "Stored embedding dimension: 119\n",
      "New query embedding dimension: 5\n"
     ]
    }
   ],
   "source": [
    "def debug_embedding_issue(store):\n",
    "    print(\"=== DEBUGGING EMBEDDING DIMENSIONS ===\")\n",
    "    \n",
    "    # Check stored embeddings\n",
    "    if store.text_chunks:\n",
    "        sample_chunk = store.text_chunks[0]\n",
    "        print(f\"Stored embedding dimension: {len(sample_chunk.embedding)}\")\n",
    "    \n",
    "    # Check new chunker\n",
    "    chunker = ProductChunkingStrategy()\n",
    "    query_chunk = ProductChunk(\n",
    "        chunk_id=\"query_1\",\n",
    "        product_id=\"query_product\",\n",
    "        chunk_type=\"text_query\",\n",
    "        content=\"nike black and white shoes under 200$\",\n",
    "        metadata={}\n",
    "    )\n",
    "    \n",
    "    text_chunks = chunker.generate_text_embeddings([query_chunk])\n",
    "    query_embedding = text_chunks[0].embedding\n",
    "    print(f\"New query embedding dimension: {len(query_embedding)}\")\n",
    "\n",
    "debug_embedding_issue(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "050bf801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Results for: 'nike black and white shoes under 200$'\n",
      "----------------------------------------------------------------------\n",
      " 1. adidas Men's Swift Run 1.0 Shoes White/White       | $60.00       | title    | 0.4233\n",
      " 2. adidas Men's Swift Run 1.0 Shoes White/White       | $60.00       | combined | 0.3334\n",
      " 3. Adidas Men's Samba OG White                        | $100.00      | title    | 0.2679\n",
      " 4. Adidas Men's Samba OG White                        | $100.00      | combined | 0.1801\n",
      " 5. Nike Women's Dunk Low Shoes                        | $120.00      | title    | 0.1554\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(ProductChunk(content=\"adidas Men's Swift Run 1.0 Shoes White/White\", chunk_id='0a7d583fd1ae44713ac323804db4dc4b', product_id='10048774599044348456', chunk_type='title', metadata={'price': '$60.00', 'source': 'adidas', 'token_count': 14, 'char_count': 44, 'chunk_type': 'title'}),\n",
       "  np.float64(0.42329711595997044)),\n",
       " (ProductChunk(content=\"Title: adidas Men's Swift Run 1.0 Shoes White/White | Price: $60.00 | Available at: adidas | Rating: 4.2 | Reviews: 29\", chunk_id='c905d928279a6fa66a8b0b1597b13617', product_id='10048774599044348456', chunk_type='combined', metadata={'price': '$60.00', 'source': 'adidas', 'link': 'https://www.google.com/shopping/product/10048774599044348456?gl=us', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/shopping?q=tbn:ANd9GcSq2C61VmHnVDsCY34La09auIgaIRO9US4eHL0Ykuh4Y8WnjuRf-9MDu6f_p93_xNGnbBuE1mQqwZwDmL-epbxvCW2esStkibPJmXJcYBuuGuO-RRzpJj7wfA', 'token_count': 39, 'char_count': 118, 'chunk_type': 'combined'}),\n",
       "  np.float64(0.33336518267174625)),\n",
       " (ProductChunk(content=\"Adidas Men's Samba OG White\", chunk_id='9c1ead13f23c98eb4e550fb066938546', product_id='6745863268625178139', chunk_type='title', metadata={'price': '$100.00', 'source': 'adidas', 'token_count': 8, 'char_count': 27, 'chunk_type': 'title'}),\n",
       "  np.float64(0.2679289307635974)),\n",
       " (ProductChunk(content=\"Title: Adidas Men's Samba OG White | Price: $100.00 | Available at: adidas | Rating: 4.8 | Reviews: 18000\", chunk_id='f39bbae2f4409b0c79cc5ecdd1d6490c', product_id='6745863268625178139', chunk_type='combined', metadata={'price': '$100.00', 'source': 'adidas', 'link': 'https://www.google.com/shopping/product/6745863268625178139?gl=us', 'thumbnail': 'https://serpapi.com/searches/68b3b9d8e396eb31b38b13c5/images/b6d85c4a0ae9dbf63a9d5fd75ad6ad22f5b4ba9ed2819fa767fd4a1493e607b3.webp', 'token_count': 34, 'char_count': 105, 'chunk_type': 'combined'}),\n",
       "  np.float64(0.18010583174475775)),\n",
       " (ProductChunk(content=\"Nike Women's Dunk Low Shoes\", chunk_id='3487a5ddd6ad40fff2a0ecdaf47f132c', product_id='903764150221245935', chunk_type='title', metadata={'price': '$120.00', 'source': 'Nike', 'token_count': 6, 'char_count': 27, 'chunk_type': 'title'}),\n",
       "  np.float64(0.15542956181881756))]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multimodal_search_working(store, text_query: str, image_path: str = None, top_k: int = 5):\n",
    "    \"\"\"Working version using the existing search method\"\"\"\n",
    "    \n",
    "    # For now, use text-only search since it works\n",
    "    results = store.search_similar_text(text_query, top_k=top_k)\n",
    "    \n",
    "    print(f\"\\n🔎 Results for: '{text_query}'\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, (chunk, score) in enumerate(results, 1):\n",
    "        # Find product\n",
    "        product = None\n",
    "        for prod in store.products_metadata:\n",
    "            prod_id = prod.get('id') or prod.get('product_id') \n",
    "            if str(prod_id) == str(chunk.product_id):\n",
    "                product = prod\n",
    "                break\n",
    "        \n",
    "        if product is None:\n",
    "            product = {'title': f\"Product {chunk.product_id}\", 'price': 'N/A'}\n",
    "        \n",
    "        title = product.get('title', 'Unknown')[:50]\n",
    "        price = product.get('price', 'N/A')\n",
    "        chunk_type = chunk.chunk_type\n",
    "        \n",
    "        print(f\"{i:2d}. {title:<50} | {price:<12} | {chunk_type:<8} | {score:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    return results\n",
    "\n",
    "# Test the working version\n",
    "multimodal_search_working(store, \"nike black and white shoes under 200$\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "20189f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Image embedding generated from query_test.webp\n",
      "⚠️ Dimension mismatch: text=5, image=96. Using text only.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 5 while Y.shape[1] == 119",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m multimodal_search(\n\u001b[0;32m      2\u001b[0m     store,\n\u001b[0;32m      3\u001b[0m     text_query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnike black and white shoes under 200$\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     image_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_test.webp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n",
      "Cell \u001b[1;32mIn[80], line 46\u001b[0m, in \u001b[0;36mmultimodal_search\u001b[1;34m(store, text_query, image_path, top_k, alpha)\u001b[0m\n\u001b[0;32m     43\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m text_embedding\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Step 4: Query store\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m results \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mquery(query_embedding, top_k\u001b[38;5;241m=\u001b[39mtop_k)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Step 5: Print results\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🔎 Results for query: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m + image=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mbool\u001b[39m(image_path\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mand\u001b[39;00m\u001b[38;5;250m \u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(image_path))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[77], line 112\u001b[0m, in \u001b[0;36mProductEmbeddingStore.query\u001b[1;34m(self, query_embedding, top_k)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_chunks:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;66;03m# Calculate cosine similarity between query and stored embeddings\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m         sim \u001b[38;5;241m=\u001b[39m cosine_similarity([query_embedding], [chunk\u001b[38;5;241m.\u001b[39membedding])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    113\u001b[0m         similarities\u001b[38;5;241m.\u001b[39mappend((chunk, sim))\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Sort by similarity (highest first) and return top results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\divya\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\divya\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1741\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \n\u001b[0;32m   1697\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1741\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[0;32m   1743\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\divya\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:229\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputed metric requires shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n_queries, n_indexed). Got (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m indexed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    225\u001b[0m         )\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Only check the number of features if 2d arrays are enforced. Otherwise,\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# validation is left to the user for custom metrics.\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m while Y.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    232\u001b[0m     )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 5 while Y.shape[1] == 119"
     ]
    }
   ],
   "source": [
    "results = multimodal_search(\n",
    "    store,\n",
    "    text_query=\"nike black and white shoes under 200$\",\n",
    "    image_path=\"query_test.webp\",\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c5e80a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting product processing pipeline...\n",
      "\n",
      "📱 Fetching products for: 'nike shoes'\n",
      "   Found 40 products\n",
      "   Generated 80 text chunks with embeddings\n",
      "   Processed 40 product images\n",
      "Added 40 products, 80 text chunks, 40 image chunks\n",
      "   ✅ Completed processing for 'nike shoes'\n",
      "\n",
      "📱 Fetching products for: 'adidas shoes'\n",
      "   Found 40 products\n",
      "   Generated 80 text chunks with embeddings\n",
      "   Processed 40 product images\n",
      "Added 40 products, 80 text chunks, 40 image chunks\n",
      "   ✅ Completed processing for 'adidas shoes'\n",
      "\n",
      "📱 Fetching products for: 'jackets'\n",
      "   Found 40 products\n",
      "   Generated 80 text chunks with embeddings\n",
      "   Processed 40 product images\n",
      "Added 40 products, 80 text chunks, 40 image chunks\n",
      "   ✅ Completed processing for 'jackets'\n",
      "\n",
      "Saved data to product_embeddings\n",
      "📊 Pipeline Statistics:\n",
      "{\n",
      "  \"total_products\": 120,\n",
      "  \"total_text_chunks\": 240,\n",
      "  \"total_image_chunks\": 120,\n",
      "  \"chunk_types\": {\n",
      "    \"title\": 120,\n",
      "    \"combined\": 120\n",
      "  },\n",
      "  \"avg_embedding_size\": 141.33333333333334\n",
      "}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 5 while Y.shape[1] == 119",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m store \u001b[38;5;241m=\u001b[39m test_product_pipeline(queries, num_products_per_query\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Now run multimodal query\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m multimodal_search(store, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnike black and white shoes under 200$\u001b[39m\u001b[38;5;124m\"\u001b[39m, image_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_test.webp\u001b[39m\u001b[38;5;124m\"\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[68], line 31\u001b[0m, in \u001b[0;36mmultimodal_search\u001b[1;34m(store, text_query, image_path, top_k, alpha)\u001b[0m\n\u001b[0;32m     28\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m text_embedding\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Step 4: Query store\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m results \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mquery(query_embedding, top_k\u001b[38;5;241m=\u001b[39mtop_k)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Step 5: Print results\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🔎 Results for query: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m + image=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mbool\u001b[39m(image_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[77], line 112\u001b[0m, in \u001b[0;36mProductEmbeddingStore.query\u001b[1;34m(self, query_embedding, top_k)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_chunks:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;66;03m# Calculate cosine similarity between query and stored embeddings\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m         sim \u001b[38;5;241m=\u001b[39m cosine_similarity([query_embedding], [chunk\u001b[38;5;241m.\u001b[39membedding])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    113\u001b[0m         similarities\u001b[38;5;241m.\u001b[39mappend((chunk, sim))\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Sort by similarity (highest first) and return top results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\divya\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\divya\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1741\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \n\u001b[0;32m   1697\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1741\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[0;32m   1743\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\divya\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:229\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputed metric requires shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n_queries, n_indexed). Got (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m indexed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    225\u001b[0m         )\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Only check the number of features if 2d arrays are enforced. Otherwise,\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# validation is left to the user for custom metrics.\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m while Y.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    232\u001b[0m     )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 5 while Y.shape[1] == 119"
     ]
    }
   ],
   "source": [
    "# Run the pipeline first\n",
    "queries = [\"nike shoes\", \"adidas shoes\", \"jackets\"]\n",
    "store = test_product_pipeline(queries, num_products_per_query=5)\n",
    "\n",
    "# Now run multimodal query\n",
    "multimodal_search(store, \"nike black and white shoes under 200$\", image_path=\"query_test.webp\", top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f5934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73793e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
